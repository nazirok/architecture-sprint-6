
==> Audit <==
|-----------|---------------------------|----------|---------|---------|---------------------|---------------------|
|  Command  |           Args            | Profile  |  User   | Version |     Start Time      |      End Time       |
|-----------|---------------------------|----------|---------|---------|---------------------|---------------------|
| kubectl   | -- get pods               | minikube | nazirok | v1.34.0 | 04 Jan 25 17:11 MSK | 04 Jan 25 17:11 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:18 MSK | 04 Jan 25 17:18 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:18 MSK | 04 Jan 25 17:18 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:19 MSK | 04 Jan 25 17:19 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:20 MSK | 04 Jan 25 17:20 MSK |
| dashboard |                           | minikube | nazirok | v1.34.0 | 04 Jan 25 17:24 MSK |                     |
| addons    | enable metrics-server     | minikube | nazirok | v1.34.0 | 04 Jan 25 17:25 MSK | 04 Jan 25 17:25 MSK |
| kubectl   | -- get pods               | minikube | nazirok | v1.34.0 | 04 Jan 25 17:38 MSK | 04 Jan 25 17:38 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:38 MSK | 04 Jan 25 17:38 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:40 MSK | 04 Jan 25 17:40 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:40 MSK | 04 Jan 25 17:40 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:40 MSK | 04 Jan 25 17:40 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:41 MSK | 04 Jan 25 17:41 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:41 MSK | 04 Jan 25 17:41 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:42 MSK | 04 Jan 25 17:42 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:42 MSK | 04 Jan 25 17:42 MSK |
| kubectl   | -- get svc                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:45 MSK | 04 Jan 25 17:45 MSK |
| kubectl   | -- delete svc testapp     | minikube | nazirok | v1.34.0 | 04 Jan 25 17:45 MSK | 04 Jan 25 17:45 MSK |
| kubectl   | -- apply -f service.yaml  | minikube | nazirok | v1.34.0 | 04 Jan 25 17:46 MSK | 04 Jan 25 17:46 MSK |
| kubectl   | -- get svc                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:46 MSK | 04 Jan 25 17:46 MSK |
| service   | testapp --url             | minikube | nazirok | v1.34.0 | 04 Jan 25 17:47 MSK | 04 Jan 25 17:47 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:49 MSK | 04 Jan 25 17:49 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:49 MSK | 04 Jan 25 17:49 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:50 MSK | 04 Jan 25 17:50 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:50 MSK | 04 Jan 25 17:50 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:50 MSK | 04 Jan 25 17:50 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:51 MSK | 04 Jan 25 17:51 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:51 MSK | 04 Jan 25 17:51 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:51 MSK | 04 Jan 25 17:51 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:51 MSK | 04 Jan 25 17:51 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:52 MSK | 04 Jan 25 17:52 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:52 MSK | 04 Jan 25 17:52 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:52 MSK | 04 Jan 25 17:52 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:52 MSK | 04 Jan 25 17:52 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:52 MSK | 04 Jan 25 17:52 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:53 MSK | 04 Jan 25 17:53 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:53 MSK | 04 Jan 25 17:53 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:53 MSK | 04 Jan 25 17:53 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:54 MSK | 04 Jan 25 17:54 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:54 MSK | 04 Jan 25 17:54 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:54 MSK | 04 Jan 25 17:54 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:54 MSK | 04 Jan 25 17:54 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:56 MSK | 04 Jan 25 17:56 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:56 MSK | 04 Jan 25 17:56 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:56 MSK | 04 Jan 25 17:56 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:56 MSK | 04 Jan 25 17:56 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:56 MSK | 04 Jan 25 17:56 MSK |
| kubectl   | -- get deploy             | minikube | nazirok | v1.34.0 | 04 Jan 25 17:57 MSK | 04 Jan 25 17:57 MSK |
| kubectl   | -- get svc                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:57 MSK | 04 Jan 25 17:57 MSK |
| kubectl   | -- get hps                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:58 MSK |                     |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:58 MSK | 04 Jan 25 17:58 MSK |
| kubectl   | -- delete hpa testapp-hpa | minikube | nazirok | v1.34.0 | 04 Jan 25 17:58 MSK | 04 Jan 25 17:58 MSK |
| kubectl   | -- apply -f hpa.yaml      | minikube | nazirok | v1.34.0 | 04 Jan 25 17:58 MSK | 04 Jan 25 17:58 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:58 MSK | 04 Jan 25 17:58 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:59 MSK | 04 Jan 25 17:59 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:59 MSK | 04 Jan 25 17:59 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:59 MSK | 04 Jan 25 17:59 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 17:59 MSK | 04 Jan 25 17:59 MSK |
| kubectl   | -- get hpa                | minikube | nazirok | v1.34.0 | 04 Jan 25 18:00 MSK | 04 Jan 25 18:00 MSK |
| kubectl   | -- logs                   | minikube | nazirok | v1.34.0 | 04 Jan 25 18:03 MSK |                     |
|-----------|---------------------------|----------|---------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/01/04 12:44:56
Running on machine: nazirok-homepc
Binary: Built with gc go1.22.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0104 12:44:56.469611   11869 out.go:345] Setting OutFile to fd 1 ...
I0104 12:44:56.469744   11869 out.go:397] isatty.IsTerminal(1) = true
I0104 12:44:56.469749   11869 out.go:358] Setting ErrFile to fd 2...
I0104 12:44:56.469756   11869 out.go:397] isatty.IsTerminal(2) = true
I0104 12:44:56.470093   11869 root.go:338] Updating PATH: /home/nazirok/.minikube/bin
I0104 12:44:56.471845   11869 out.go:352] Setting JSON to false
I0104 12:44:56.489808   11869 start.go:129] hostinfo: {"hostname":"nazirok-homepc","uptime":5766,"bootTime":1735978130,"procs":288,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.0-53-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"fafdecf7-b0af-4709-8d89-5df80ea5d0be"}
I0104 12:44:56.489897   11869 start.go:139] virtualization: kvm host
I0104 12:44:56.492326   11869 out.go:177] üòÑ  minikube v1.34.0 –Ω–∞ Ubuntu 22.04
W0104 12:44:56.496272   11869 preload.go:293] Failed to list preload files: open /home/nazirok/.minikube/cache/preloaded-tarball: no such file or directory
I0104 12:44:56.496338   11869 notify.go:220] Checking for updates...
I0104 12:44:56.497473   11869 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0104 12:44:56.497604   11869 driver.go:394] Setting default libvirt URI to qemu:///system
I0104 12:44:56.558985   11869 docker.go:123] docker version: linux-20.10.22:Docker Engine - Community
I0104 12:44:56.559124   11869 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0104 12:44:57.126798   11869 info.go:266] docker info: {ID:PDAZ:2DMT:CNOD:QQSU:ZKRD:4FLG:ARGN:ETY3:KSE5:HGIE:66WI:U775 Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:94 Driver:btrfs DriverStatus:[[Build Version Btrfs v5.16.2] [Library Version 102]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:40 OomKillDisable:false NGoroutines:154 SystemTime:2025-01-04 12:44:56.585311132 +0300 MSK LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:5.15.0-53-generic OperatingSystem:Ubuntu 22.04.1 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:33612894208 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:nazirok-homepc Labels:[] ExperimentalBuild:false ServerVersion:20.10.22 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:nmwtvys2xe0tahcunm0pl7cfv NodeAddr:192.168.1.113 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.1.113:2377 NodeID:nmwtvys2xe0tahcunm0pl7cfv]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:78f51771157abb6c9ed224c22013cdf09962315d Expected:78f51771157abb6c9ed224c22013cdf09962315d} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1-docker] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.14.1] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.23.0]] Warnings:<nil>}}
I0104 12:44:57.126973   11869 docker.go:318] overlay module found
I0104 12:44:57.131648   11869 out.go:177] ‚ú®  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥—Ä–∞–π–≤–µ—Ä docker –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è
I0104 12:44:57.134008   11869 start.go:297] selected driver: docker
I0104 12:44:57.134020   11869 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:8000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:localStorageCapacityIsolation Value:false}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/nazirok:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0104 12:44:57.134248   11869 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0104 12:44:57.134404   11869 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0104 12:44:57.260591   11869 info.go:266] docker info: {ID:PDAZ:2DMT:CNOD:QQSU:ZKRD:4FLG:ARGN:ETY3:KSE5:HGIE:66WI:U775 Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:94 Driver:btrfs DriverStatus:[[Build Version Btrfs v5.16.2] [Library Version 102]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:40 OomKillDisable:false NGoroutines:154 SystemTime:2025-01-04 12:44:57.172183945 +0300 MSK LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:5.15.0-53-generic OperatingSystem:Ubuntu 22.04.1 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:33612894208 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:nazirok-homepc Labels:[] ExperimentalBuild:false ServerVersion:20.10.22 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:nmwtvys2xe0tahcunm0pl7cfv NodeAddr:192.168.1.113 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.1.113:2377 NodeID:nmwtvys2xe0tahcunm0pl7cfv]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:78f51771157abb6c9ed224c22013cdf09962315d Expected:78f51771157abb6c9ed224c22013cdf09962315d} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1-docker] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.14.1] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.23.0]] Warnings:<nil>}}
W0104 12:44:57.260731   11869 out.go:270] ‚ùó  docker is currently using the btrfs storage driver, setting preload=false
I0104 12:44:57.285144   11869 cni.go:84] Creating CNI manager for ""
I0104 12:44:57.285163   11869 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0104 12:44:57.285245   11869 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:8000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:localStorageCapacityIsolation Value:false}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/nazirok:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0104 12:44:57.289378   11869 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0104 12:44:57.291466   11869 cache.go:121] Beginning downloading kic base image for docker with docker
I0104 12:44:57.293896   11869 out.go:177] üöú  Pulling base image v0.0.45 ...
I0104 12:44:57.296336   11869 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0104 12:44:57.296434   11869 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I0104 12:44:57.296543   11869 profile.go:143] Saving config to /home/nazirok/.minikube/profiles/minikube/config.json ...
I0104 12:44:57.296651   11869 cache.go:107] acquiring lock: {Name:mk6d4664871e88696d3b3f5f77557e8d4ef8c43b Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0104 12:44:57.296812   11869 cache.go:107] acquiring lock: {Name:mk785228fdf27dddf3baa7189148b4ca0308c3ff Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0104 12:44:57.297051   11869 cache.go:115] /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-proxy_v1.31.0 exists
I0104 12:44:57.297069   11869 cache.go:96] cache image "registry.k8s.io/kube-proxy:v1.31.0" -> "/home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-proxy_v1.31.0" took 266.504¬µs
I0104 12:44:57.297083   11869 cache.go:80] save to tar file registry.k8s.io/kube-proxy:v1.31.0 -> /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-proxy_v1.31.0 succeeded
I0104 12:44:57.297107   11869 cache.go:107] acquiring lock: {Name:mk01f3f28ce99f9d4f3cfee5ba761b4539091dc3 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0104 12:44:57.297245   11869 cache.go:115] /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-apiserver_v1.31.0 exists
I0104 12:44:57.297259   11869 cache.go:96] cache image "registry.k8s.io/kube-apiserver:v1.31.0" -> "/home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-apiserver_v1.31.0" took 157.168¬µs
I0104 12:44:57.297270   11869 cache.go:80] save to tar file registry.k8s.io/kube-apiserver:v1.31.0 -> /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-apiserver_v1.31.0 succeeded
I0104 12:44:57.297292   11869 cache.go:107] acquiring lock: {Name:mk05fe7bc0afe871ae6bb456d5d5100d20db708a Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0104 12:44:57.297440   11869 cache.go:115] /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-controller-manager_v1.31.0 exists
I0104 12:44:57.297448   11869 cache.go:115] /home/nazirok/.minikube/cache/images/amd64/gcr.io/k8s-minikube/storage-provisioner_v5 exists
I0104 12:44:57.297453   11869 cache.go:96] cache image "registry.k8s.io/kube-controller-manager:v1.31.0" -> "/home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-controller-manager_v1.31.0" took 164.99¬µs
I0104 12:44:57.297464   11869 cache.go:80] save to tar file registry.k8s.io/kube-controller-manager:v1.31.0 -> /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-controller-manager_v1.31.0 succeeded
I0104 12:44:57.297468   11869 cache.go:96] cache image "gcr.io/k8s-minikube/storage-provisioner:v5" -> "/home/nazirok/.minikube/cache/images/amd64/gcr.io/k8s-minikube/storage-provisioner_v5" took 826.216¬µs
I0104 12:44:57.297483   11869 cache.go:80] save to tar file gcr.io/k8s-minikube/storage-provisioner:v5 -> /home/nazirok/.minikube/cache/images/amd64/gcr.io/k8s-minikube/storage-provisioner_v5 succeeded
I0104 12:44:57.297485   11869 cache.go:107] acquiring lock: {Name:mk78b454921b629c3de1907944ea3661f31e0d05 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0104 12:44:57.297511   11869 cache.go:107] acquiring lock: {Name:mkefea69ebe643b183d31502cc3eed230db43c9e Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0104 12:44:57.297554   11869 cache.go:107] acquiring lock: {Name:mkffb52a72ebb6346253b5ce403295ca1b0f61f5 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0104 12:44:57.297624   11869 cache.go:115] /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-scheduler_v1.31.0 exists
I0104 12:44:57.297638   11869 cache.go:96] cache image "registry.k8s.io/kube-scheduler:v1.31.0" -> "/home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-scheduler_v1.31.0" took 156.037¬µs
I0104 12:44:57.297648   11869 cache.go:80] save to tar file registry.k8s.io/kube-scheduler:v1.31.0 -> /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-scheduler_v1.31.0 succeeded
I0104 12:44:57.297670   11869 cache.go:115] /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/pause_3.10 exists
I0104 12:44:57.297684   11869 cache.go:96] cache image "registry.k8s.io/pause:3.10" -> "/home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/pause_3.10" took 178.423¬µs
I0104 12:44:57.297675   11869 cache.go:107] acquiring lock: {Name:mk21616662b8167262aaa95a38d96f593ef79873 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0104 12:44:57.297694   11869 cache.go:80] save to tar file registry.k8s.io/pause:3.10 -> /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/pause_3.10 succeeded
I0104 12:44:57.297740   11869 cache.go:115] /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/etcd_3.5.15-0 exists
I0104 12:44:57.297756   11869 cache.go:96] cache image "registry.k8s.io/etcd:3.5.15-0" -> "/home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/etcd_3.5.15-0" took 212.446¬µs
I0104 12:44:57.297769   11869 cache.go:80] save to tar file registry.k8s.io/etcd:3.5.15-0 -> /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/etcd_3.5.15-0 succeeded
I0104 12:44:57.297806   11869 cache.go:115] /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/coredns/coredns_v1.11.1 exists
I0104 12:44:57.297818   11869 cache.go:96] cache image "registry.k8s.io/coredns/coredns:v1.11.1" -> "/home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/coredns/coredns_v1.11.1" took 148.743¬µs
I0104 12:44:57.297828   11869 cache.go:80] save to tar file registry.k8s.io/coredns/coredns:v1.11.1 -> /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/coredns/coredns_v1.11.1 succeeded
I0104 12:44:57.297843   11869 cache.go:87] Successfully saved all images to host disk.
W0104 12:44:57.387673   11869 image.go:95] image gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I0104 12:44:57.387681   11869 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I0104 12:44:57.387794   11869 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I0104 12:44:57.387833   11869 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I0104 12:44:57.387837   11869 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I0104 12:44:57.387844   11869 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I0104 12:44:57.387849   11869 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I0104 12:44:57.593833   11869 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I0104 12:44:57.593882   11869 cache.go:194] Successfully downloaded all kic artifacts
I0104 12:44:57.593964   11869 start.go:360] acquireMachinesLock for minikube: {Name:mk3605699087af7415aeaf0afd380a34a763aa76 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0104 12:44:57.594181   11869 start.go:364] duration metric: took 183.259¬µs to acquireMachinesLock for "minikube"
I0104 12:44:57.594222   11869 start.go:96] Skipping create...Using existing machine configuration
I0104 12:44:57.594252   11869 fix.go:54] fixHost starting: 
I0104 12:44:57.594942   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0104 12:44:57.646869   11869 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0104 12:44:57.646911   11869 fix.go:112] recreateIfNeeded on minikube: state= err=unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:44:57.646930   11869 fix.go:117] machineExists: false. err=machine does not exist
I0104 12:44:57.650848   11869 out.go:177] ü§∑  docker "minikube" container is missing, will recreate.
I0104 12:44:57.652877   11869 delete.go:124] DEMOLISHING minikube ...
I0104 12:44:57.652967   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0104 12:44:57.675649   11869 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0104 12:44:57.675686   11869 stop.go:83] unable to get state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:44:57.675701   11869 delete.go:128] stophost failed (probably ok): ssh power off: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:44:57.676114   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0104 12:44:57.698773   11869 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0104 12:44:57.698817   11869 delete.go:82] Unable to get host status for minikube, assuming it has already been deleted: state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:44:57.698876   11869 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0104 12:44:57.721420   11869 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0104 12:44:57.721438   11869 kic.go:371] could not find the container minikube to remove it. will try anyways
I0104 12:44:57.721483   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0104 12:44:57.743874   11869 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0104 12:44:57.743910   11869 oci.go:84] error getting container status, will try to delete anyways: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:44:57.743958   11869 cli_runner.go:164] Run: docker exec --privileged -t minikube /bin/bash -c "sudo init 0"
W0104 12:44:57.766426   11869 cli_runner.go:211] docker exec --privileged -t minikube /bin/bash -c "sudo init 0" returned with exit code 1
I0104 12:44:57.766454   11869 oci.go:650] error shutdown minikube: docker exec --privileged -t minikube /bin/bash -c "sudo init 0": exit status 1
stdout:

stderr:
Error: No such container: minikube
I0104 12:44:58.766783   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0104 12:44:58.787284   11869 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0104 12:44:58.787321   11869 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:44:58.787325   11869 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I0104 12:44:58.787343   11869 retry.go:31] will retry after 281.425686ms: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:44:59.069756   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0104 12:44:59.091140   11869 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0104 12:44:59.091177   11869 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:44:59.091183   11869 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I0104 12:44:59.091202   11869 retry.go:31] will retry after 1.08130343s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:45:00.172882   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0104 12:45:00.193199   11869 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0104 12:45:00.193235   11869 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:45:00.193239   11869 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I0104 12:45:00.193255   11869 retry.go:31] will retry after 661.298721ms: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:45:00.854937   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0104 12:45:00.875146   11869 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0104 12:45:00.875183   11869 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:45:00.875187   11869 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I0104 12:45:00.875204   11869 retry.go:31] will retry after 1.665980896s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:45:02.542239   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0104 12:45:02.563604   11869 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0104 12:45:02.563643   11869 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:45:02.563651   11869 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I0104 12:45:02.563669   11869 retry.go:31] will retry after 2.631921461s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:45:05.196688   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0104 12:45:05.216966   11869 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0104 12:45:05.216998   11869 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:45:05.217002   11869 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I0104 12:45:05.217018   11869 retry.go:31] will retry after 2.545212242s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:45:07.763336   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0104 12:45:07.783484   11869 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0104 12:45:07.783515   11869 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:45:07.783520   11869 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I0104 12:45:07.783550   11869 retry.go:31] will retry after 7.392293201s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:45:15.177765   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0104 12:45:15.197910   11869 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0104 12:45:15.197942   11869 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I0104 12:45:15.197946   11869 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I0104 12:45:15.197973   11869 oci.go:88] couldn't shut down minikube (might be okay): verify shutdown: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
 
I0104 12:45:15.198008   11869 cli_runner.go:164] Run: docker rm -f -v minikube
I0104 12:45:15.217674   11869 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0104 12:45:15.237264   11869 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0104 12:45:15.237316   11869 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0104 12:45:15.258466   11869 cli_runner.go:164] Run: docker network rm minikube
I0104 12:45:15.383723   11869 fix.go:124] Sleeping 1 second for extra luck!
I0104 12:45:16.383859   11869 start.go:125] createHost starting for "" (driver="docker")
I0104 12:45:16.389892   11869 out.go:235] üî•  Creating docker container (CPUs=2, Memory=8000MB) ...
I0104 12:45:16.390018   11869 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0104 12:45:16.390037   11869 client.go:168] LocalClient.Create starting
I0104 12:45:16.390141   11869 main.go:141] libmachine: Reading certificate data from /home/nazirok/.minikube/certs/ca.pem
I0104 12:45:16.390214   11869 main.go:141] libmachine: Decoding PEM data...
I0104 12:45:16.390228   11869 main.go:141] libmachine: Parsing certificate...
I0104 12:45:16.390292   11869 main.go:141] libmachine: Reading certificate data from /home/nazirok/.minikube/certs/cert.pem
I0104 12:45:16.390334   11869 main.go:141] libmachine: Decoding PEM data...
I0104 12:45:16.390345   11869 main.go:141] libmachine: Parsing certificate...
I0104 12:45:16.390599   11869 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0104 12:45:16.411721   11869 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0104 12:45:16.411779   11869 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0104 12:45:16.411791   11869 cli_runner.go:164] Run: docker network inspect minikube
W0104 12:45:16.434920   11869 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0104 12:45:16.434938   11869 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error: No such network: minikube
I0104 12:45:16.434947   11869 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error: No such network: minikube

** /stderr **
I0104 12:45:16.435035   11869 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0104 12:45:16.458192   11869 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001933440}
I0104 12:45:16.458219   11869 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0104 12:45:16.458261   11869 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0104 12:45:16.527559   11869 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0104 12:45:16.527571   11869 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0104 12:45:16.527619   11869 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0104 12:45:16.544253   11869 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0104 12:45:16.563962   11869 oci.go:103] Successfully created a docker volume minikube
I0104 12:45:16.564020   11869 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -d /var/lib
I0104 12:45:17.723071   11869 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -d /var/lib: (1.159029889s)
I0104 12:45:17.723095   11869 oci.go:107] Successfully prepared a docker volume minikube
I0104 12:45:17.723123   11869 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
W0104 12:45:17.723192   11869 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0104 12:45:17.723221   11869 oci.go:243] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0104 12:45:17.723256   11869 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0104 12:45:17.795664   11869 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=8000mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85
I0104 12:45:18.813975   11869 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=8000mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85: (1.0182599s)
I0104 12:45:18.814057   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0104 12:45:18.844783   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0104 12:45:18.864365   11869 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0104 12:45:18.939790   11869 oci.go:144] the created container "minikube" has a running status.
I0104 12:45:18.939800   11869 kic.go:225] Creating ssh key for kic: /home/nazirok/.minikube/machines/minikube/id_rsa...
I0104 12:45:19.292511   11869 kic_runner.go:191] docker (temp): /home/nazirok/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0104 12:45:19.421194   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0104 12:45:19.439719   11869 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0104 12:45:19.439727   11869 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0104 12:45:19.512834   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0104 12:45:19.531189   11869 machine.go:93] provisionDockerMachine start ...
I0104 12:45:19.531252   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:19.549286   11869 main.go:141] libmachine: Using SSH client type: native
I0104 12:45:19.549439   11869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0104 12:45:19.549444   11869 main.go:141] libmachine: About to run SSH command:
hostname
I0104 12:45:19.733221   11869 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0104 12:45:19.733242   11869 ubuntu.go:169] provisioning hostname "minikube"
I0104 12:45:19.733337   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:19.776651   11869 main.go:141] libmachine: Using SSH client type: native
I0104 12:45:19.776864   11869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0104 12:45:19.776875   11869 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0104 12:45:19.932025   11869 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0104 12:45:19.932070   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:19.950784   11869 main.go:141] libmachine: Using SSH client type: native
I0104 12:45:19.950933   11869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0104 12:45:19.950945   11869 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0104 12:45:20.075543   11869 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0104 12:45:20.075555   11869 ubuntu.go:175] set auth options {CertDir:/home/nazirok/.minikube CaCertPath:/home/nazirok/.minikube/certs/ca.pem CaPrivateKeyPath:/home/nazirok/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/nazirok/.minikube/machines/server.pem ServerKeyPath:/home/nazirok/.minikube/machines/server-key.pem ClientKeyPath:/home/nazirok/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/nazirok/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/nazirok/.minikube}
I0104 12:45:20.075576   11869 ubuntu.go:177] setting up certificates
I0104 12:45:20.075581   11869 provision.go:84] configureAuth start
I0104 12:45:20.075621   11869 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0104 12:45:20.097772   11869 provision.go:143] copyHostCerts
I0104 12:45:20.097834   11869 exec_runner.go:144] found /home/nazirok/.minikube/key.pem, removing ...
I0104 12:45:20.097839   11869 exec_runner.go:203] rm: /home/nazirok/.minikube/key.pem
I0104 12:45:20.098196   11869 exec_runner.go:151] cp: /home/nazirok/.minikube/certs/key.pem --> /home/nazirok/.minikube/key.pem (1679 bytes)
I0104 12:45:20.098279   11869 exec_runner.go:144] found /home/nazirok/.minikube/ca.pem, removing ...
I0104 12:45:20.098282   11869 exec_runner.go:203] rm: /home/nazirok/.minikube/ca.pem
I0104 12:45:20.098607   11869 exec_runner.go:151] cp: /home/nazirok/.minikube/certs/ca.pem --> /home/nazirok/.minikube/ca.pem (1082 bytes)
I0104 12:45:20.098670   11869 exec_runner.go:144] found /home/nazirok/.minikube/cert.pem, removing ...
I0104 12:45:20.098674   11869 exec_runner.go:203] rm: /home/nazirok/.minikube/cert.pem
I0104 12:45:20.098703   11869 exec_runner.go:151] cp: /home/nazirok/.minikube/certs/cert.pem --> /home/nazirok/.minikube/cert.pem (1123 bytes)
I0104 12:45:20.098749   11869 provision.go:117] generating server cert: /home/nazirok/.minikube/machines/server.pem ca-key=/home/nazirok/.minikube/certs/ca.pem private-key=/home/nazirok/.minikube/certs/ca-key.pem org=nazirok.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0104 12:45:20.247811   11869 provision.go:177] copyRemoteCerts
I0104 12:45:20.247871   11869 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0104 12:45:20.247900   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:20.267005   11869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nazirok/.minikube/machines/minikube/id_rsa Username:docker}
I0104 12:45:20.375300   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0104 12:45:20.447647   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I0104 12:45:20.483362   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0104 12:45:20.515391   11869 provision.go:87] duration metric: took 439.801103ms to configureAuth
I0104 12:45:20.515406   11869 ubuntu.go:193] setting minikube options for container-runtime
I0104 12:45:20.515586   11869 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0104 12:45:20.515640   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:20.544267   11869 main.go:141] libmachine: Using SSH client type: native
I0104 12:45:20.544494   11869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0104 12:45:20.544504   11869 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0104 12:45:20.718547   11869 main.go:141] libmachine: SSH cmd err, output: <nil>: btrfs

I0104 12:45:20.718577   11869 ubuntu.go:71] root file system type: btrfs
I0104 12:45:20.718792   11869 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0104 12:45:20.718926   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:20.771760   11869 main.go:141] libmachine: Using SSH client type: native
I0104 12:45:20.771928   11869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0104 12:45:20.772002   11869 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0104 12:45:20.986892   11869 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0104 12:45:20.987006   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:21.014316   11869 main.go:141] libmachine: Using SSH client type: native
I0104 12:45:21.014472   11869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0104 12:45:21.014485   11869 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0104 12:45:22.557128   11869 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-08-27 14:13:43.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-01-04 09:45:20.978570062 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0104 12:45:22.557143   11869 machine.go:96] duration metric: took 3.02594471s to provisionDockerMachine
I0104 12:45:22.557153   11869 client.go:171] duration metric: took 6.167111289s to LocalClient.Create
I0104 12:45:22.557166   11869 start.go:167] duration metric: took 6.167149863s to libmachine.API.Create "minikube"
I0104 12:45:22.557172   11869 start.go:293] postStartSetup for "minikube" (driver="docker")
I0104 12:45:22.557181   11869 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0104 12:45:22.557240   11869 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0104 12:45:22.557284   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:22.586003   11869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nazirok/.minikube/machines/minikube/id_rsa Username:docker}
I0104 12:45:22.706441   11869 ssh_runner.go:195] Run: cat /etc/os-release
I0104 12:45:22.710666   11869 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0104 12:45:22.710699   11869 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0104 12:45:22.710711   11869 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0104 12:45:22.710717   11869 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0104 12:45:22.710727   11869 filesync.go:126] Scanning /home/nazirok/.minikube/addons for local assets ...
I0104 12:45:22.710799   11869 filesync.go:126] Scanning /home/nazirok/.minikube/files for local assets ...
I0104 12:45:22.710833   11869 start.go:296] duration metric: took 153.655745ms for postStartSetup
I0104 12:45:22.711314   11869 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0104 12:45:22.733161   11869 profile.go:143] Saving config to /home/nazirok/.minikube/profiles/minikube/config.json ...
I0104 12:45:22.733505   11869 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0104 12:45:22.733542   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:22.754372   11869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nazirok/.minikube/machines/minikube/id_rsa Username:docker}
I0104 12:45:22.858473   11869 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0104 12:45:22.871915   11869 start.go:128] duration metric: took 6.488032459s to createHost
I0104 12:45:22.872120   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0104 12:45:22.937799   11869 fix.go:138] unexpected machine state, will restart: <nil>
I0104 12:45:22.937825   11869 machine.go:93] provisionDockerMachine start ...
I0104 12:45:22.937982   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:22.968040   11869 main.go:141] libmachine: Using SSH client type: native
I0104 12:45:22.968191   11869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0104 12:45:22.968197   11869 main.go:141] libmachine: About to run SSH command:
hostname
I0104 12:45:23.129673   11869 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0104 12:45:23.129699   11869 ubuntu.go:169] provisioning hostname "minikube"
I0104 12:45:23.129840   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:23.185030   11869 main.go:141] libmachine: Using SSH client type: native
I0104 12:45:23.185232   11869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0104 12:45:23.185243   11869 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0104 12:45:23.355502   11869 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0104 12:45:23.355653   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:23.407312   11869 main.go:141] libmachine: Using SSH client type: native
I0104 12:45:23.407559   11869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0104 12:45:23.407579   11869 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0104 12:45:23.591936   11869 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0104 12:45:23.591968   11869 ubuntu.go:175] set auth options {CertDir:/home/nazirok/.minikube CaCertPath:/home/nazirok/.minikube/certs/ca.pem CaPrivateKeyPath:/home/nazirok/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/nazirok/.minikube/machines/server.pem ServerKeyPath:/home/nazirok/.minikube/machines/server-key.pem ClientKeyPath:/home/nazirok/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/nazirok/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/nazirok/.minikube}
I0104 12:45:23.591998   11869 ubuntu.go:177] setting up certificates
I0104 12:45:23.592019   11869 provision.go:84] configureAuth start
I0104 12:45:23.592145   11869 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0104 12:45:23.639425   11869 provision.go:143] copyHostCerts
I0104 12:45:23.639471   11869 exec_runner.go:144] found /home/nazirok/.minikube/ca.pem, removing ...
I0104 12:45:23.639477   11869 exec_runner.go:203] rm: /home/nazirok/.minikube/ca.pem
I0104 12:45:23.639577   11869 exec_runner.go:151] cp: /home/nazirok/.minikube/certs/ca.pem --> /home/nazirok/.minikube/ca.pem (1082 bytes)
I0104 12:45:23.639693   11869 exec_runner.go:144] found /home/nazirok/.minikube/cert.pem, removing ...
I0104 12:45:23.639697   11869 exec_runner.go:203] rm: /home/nazirok/.minikube/cert.pem
I0104 12:45:23.639739   11869 exec_runner.go:151] cp: /home/nazirok/.minikube/certs/cert.pem --> /home/nazirok/.minikube/cert.pem (1123 bytes)
I0104 12:45:23.639840   11869 exec_runner.go:144] found /home/nazirok/.minikube/key.pem, removing ...
I0104 12:45:23.639844   11869 exec_runner.go:203] rm: /home/nazirok/.minikube/key.pem
I0104 12:45:23.639883   11869 exec_runner.go:151] cp: /home/nazirok/.minikube/certs/key.pem --> /home/nazirok/.minikube/key.pem (1679 bytes)
I0104 12:45:23.640002   11869 provision.go:117] generating server cert: /home/nazirok/.minikube/machines/server.pem ca-key=/home/nazirok/.minikube/certs/ca.pem private-key=/home/nazirok/.minikube/certs/ca-key.pem org=nazirok.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0104 12:45:23.702757   11869 provision.go:177] copyRemoteCerts
I0104 12:45:23.702790   11869 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0104 12:45:23.702821   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:23.719835   11869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nazirok/.minikube/machines/minikube/id_rsa Username:docker}
I0104 12:45:23.835720   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/machines/server.pem --> /etc/docker/server.pem (1184 bytes)
I0104 12:45:23.902327   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0104 12:45:23.955359   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0104 12:45:24.008522   11869 provision.go:87] duration metric: took 416.486257ms to configureAuth
I0104 12:45:24.008548   11869 ubuntu.go:193] setting minikube options for container-runtime
I0104 12:45:24.008852   11869 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0104 12:45:24.008943   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:24.044143   11869 main.go:141] libmachine: Using SSH client type: native
I0104 12:45:24.044295   11869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0104 12:45:24.044304   11869 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0104 12:45:24.216252   11869 main.go:141] libmachine: SSH cmd err, output: <nil>: btrfs

I0104 12:45:24.216273   11869 ubuntu.go:71] root file system type: btrfs
I0104 12:45:24.216544   11869 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0104 12:45:24.216686   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:24.272413   11869 main.go:141] libmachine: Using SSH client type: native
I0104 12:45:24.272692   11869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0104 12:45:24.272810   11869 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0104 12:45:24.491123   11869 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0104 12:45:24.491241   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:24.536550   11869 main.go:141] libmachine: Using SSH client type: native
I0104 12:45:24.536801   11869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0104 12:45:24.536825   11869 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0104 12:45:24.732040   11869 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0104 12:45:24.732065   11869 machine.go:96] duration metric: took 1.794227372s to provisionDockerMachine
I0104 12:45:24.732114   11869 start.go:293] postStartSetup for "minikube" (driver="docker")
I0104 12:45:24.732140   11869 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0104 12:45:24.732267   11869 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0104 12:45:24.732394   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:24.800685   11869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nazirok/.minikube/machines/minikube/id_rsa Username:docker}
I0104 12:45:24.940241   11869 ssh_runner.go:195] Run: cat /etc/os-release
I0104 12:45:24.949682   11869 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0104 12:45:24.949746   11869 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0104 12:45:24.949772   11869 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0104 12:45:24.949786   11869 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0104 12:45:24.949819   11869 filesync.go:126] Scanning /home/nazirok/.minikube/addons for local assets ...
I0104 12:45:24.949998   11869 filesync.go:126] Scanning /home/nazirok/.minikube/files for local assets ...
I0104 12:45:24.950075   11869 start.go:296] duration metric: took 217.94801ms for postStartSetup
I0104 12:45:24.950193   11869 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0104 12:45:24.950291   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:24.999181   11869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nazirok/.minikube/machines/minikube/id_rsa Username:docker}
I0104 12:45:25.109492   11869 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0104 12:45:25.123135   11869 fix.go:56] duration metric: took 27.528890678s for fixHost
I0104 12:45:25.123164   11869 start.go:83] releasing machines lock for "minikube", held for 27.528963414s
I0104 12:45:25.123343   11869 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0104 12:45:25.159999   11869 ssh_runner.go:195] Run: cat /version.json
I0104 12:45:25.160035   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:25.160088   11869 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0104 12:45:25.160141   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:25.178596   11869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nazirok/.minikube/machines/minikube/id_rsa Username:docker}
I0104 12:45:25.183294   11869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nazirok/.minikube/machines/minikube/id_rsa Username:docker}
I0104 12:45:25.266347   11869 ssh_runner.go:195] Run: systemctl --version
I0104 12:45:25.551527   11869 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0104 12:45:25.557048   11869 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0104 12:45:25.596089   11869 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0104 12:45:25.596175   11869 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0104 12:45:25.632418   11869 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0104 12:45:25.632433   11869 start.go:495] detecting cgroup driver to use...
I0104 12:45:25.632464   11869 detect.go:190] detected "systemd" cgroup driver on host os
I0104 12:45:25.632596   11869 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0104 12:45:25.657828   11869 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0104 12:45:25.673722   11869 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0104 12:45:25.689196   11869 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0104 12:45:25.689261   11869 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0104 12:45:25.701541   11869 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0104 12:45:25.710404   11869 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0104 12:45:25.719324   11869 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0104 12:45:25.727330   11869 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0104 12:45:25.738061   11869 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0104 12:45:25.745864   11869 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0104 12:45:25.753336   11869 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0104 12:45:25.760998   11869 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0104 12:45:25.767821   11869 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0104 12:45:25.774626   11869 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 12:45:25.868621   11869 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0104 12:45:26.086353   11869 start.go:495] detecting cgroup driver to use...
I0104 12:45:26.086378   11869 detect.go:190] detected "systemd" cgroup driver on host os
I0104 12:45:26.086422   11869 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0104 12:45:26.105102   11869 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0104 12:45:26.105148   11869 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0104 12:45:26.116266   11869 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0104 12:45:26.129397   11869 ssh_runner.go:195] Run: which cri-dockerd
I0104 12:45:26.132081   11869 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0104 12:45:26.139394   11869 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0104 12:45:26.153297   11869 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0104 12:45:26.248564   11869 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0104 12:45:26.340845   11869 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0104 12:45:26.340913   11869 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0104 12:45:26.354843   11869 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 12:45:26.482420   11869 ssh_runner.go:195] Run: sudo systemctl restart docker
I0104 12:45:27.336749   11869 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0104 12:45:27.374359   11869 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0104 12:45:27.403575   11869 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0104 12:45:27.532884   11869 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0104 12:45:27.644569   11869 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 12:45:27.736573   11869 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0104 12:45:27.747315   11869 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0104 12:45:27.756911   11869 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 12:45:27.856795   11869 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0104 12:45:28.140206   11869 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0104 12:45:28.140461   11869 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0104 12:45:28.151798   11869 start.go:563] Will wait 60s for crictl version
I0104 12:45:28.151881   11869 ssh_runner.go:195] Run: which crictl
I0104 12:45:28.157331   11869 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0104 12:45:28.300258   11869 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I0104 12:45:28.300309   11869 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0104 12:45:28.419050   11869 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0104 12:45:28.466797   11869 out.go:235] üê≥  –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è Kubernetes v1.31.0 –Ω–∞ Docker 27.2.0 ...
I0104 12:45:28.466917   11869 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0104 12:45:28.499522   11869 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0104 12:45:28.504002   11869 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0104 12:45:28.520550   11869 out.go:177]     ‚ñ™ kubelet.localStorageCapacityIsolation=false
I0104 12:45:28.522487   11869 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:8000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:localStorageCapacityIsolation Value:false}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/nazirok:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0104 12:45:28.522581   11869 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0104 12:45:28.522638   11869 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0104 12:45:28.545825   11869 docker.go:685] Got preloaded images: 
I0104 12:45:28.545837   11869 docker.go:691] registry.k8s.io/kube-apiserver:v1.31.0 wasn't preloaded
I0104 12:45:28.545843   11869 cache_images.go:88] LoadCachedImages start: [registry.k8s.io/kube-apiserver:v1.31.0 registry.k8s.io/kube-controller-manager:v1.31.0 registry.k8s.io/kube-scheduler:v1.31.0 registry.k8s.io/kube-proxy:v1.31.0 registry.k8s.io/pause:3.10 registry.k8s.io/etcd:3.5.15-0 registry.k8s.io/coredns/coredns:v1.11.1 gcr.io/k8s-minikube/storage-provisioner:v5]
I0104 12:45:28.546734   11869 image.go:135] retrieving image: gcr.io/k8s-minikube/storage-provisioner:v5
I0104 12:45:28.547012   11869 image.go:135] retrieving image: registry.k8s.io/kube-scheduler:v1.31.0
I0104 12:45:28.547470   11869 image.go:135] retrieving image: registry.k8s.io/kube-apiserver:v1.31.0
I0104 12:45:28.547804   11869 image.go:178] daemon lookup for registry.k8s.io/kube-scheduler:v1.31.0: Error response from daemon: no such image: registry.k8s.io/kube-scheduler:v1.31.0: No such image: registry.k8s.io/kube-scheduler:v1.31.0
I0104 12:45:28.547978   11869 image.go:178] daemon lookup for gcr.io/k8s-minikube/storage-provisioner:v5: Error response from daemon: no such image: gcr.io/k8s-minikube/storage-provisioner:v5: No such image: gcr.io/k8s-minikube/storage-provisioner:v5
I0104 12:45:28.548328   11869 image.go:178] daemon lookup for registry.k8s.io/kube-apiserver:v1.31.0: Error response from daemon: no such image: registry.k8s.io/kube-apiserver:v1.31.0: No such image: registry.k8s.io/kube-apiserver:v1.31.0
I0104 12:45:28.548358   11869 image.go:135] retrieving image: registry.k8s.io/kube-controller-manager:v1.31.0
I0104 12:45:28.548529   11869 image.go:135] retrieving image: registry.k8s.io/etcd:3.5.15-0
I0104 12:45:28.548980   11869 image.go:178] daemon lookup for registry.k8s.io/kube-controller-manager:v1.31.0: Error response from daemon: no such image: registry.k8s.io/kube-controller-manager:v1.31.0: No such image: registry.k8s.io/kube-controller-manager:v1.31.0
I0104 12:45:28.549112   11869 image.go:178] daemon lookup for registry.k8s.io/etcd:3.5.15-0: Error response from daemon: no such image: registry.k8s.io/etcd:3.5.15-0: No such image: registry.k8s.io/etcd:3.5.15-0
I0104 12:45:28.549202   11869 image.go:135] retrieving image: registry.k8s.io/kube-proxy:v1.31.0
I0104 12:45:28.549596   11869 image.go:135] retrieving image: registry.k8s.io/pause:3.10
I0104 12:45:28.549722   11869 image.go:135] retrieving image: registry.k8s.io/coredns/coredns:v1.11.1
I0104 12:45:28.549805   11869 image.go:178] daemon lookup for registry.k8s.io/kube-proxy:v1.31.0: Error response from daemon: no such image: registry.k8s.io/kube-proxy:v1.31.0: No such image: registry.k8s.io/kube-proxy:v1.31.0
I0104 12:45:28.550130   11869 image.go:178] daemon lookup for registry.k8s.io/pause:3.10: Error response from daemon: no such image: registry.k8s.io/pause:3.10: No such image: registry.k8s.io/pause:3.10
I0104 12:45:28.551019   11869 image.go:178] daemon lookup for registry.k8s.io/coredns/coredns:v1.11.1: Error response from daemon: no such image: registry.k8s.io/coredns/coredns:v1.11.1: No such image: registry.k8s.io/coredns/coredns:v1.11.1
I0104 12:45:29.186574   11869 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.k8s.io/kube-controller-manager:v1.31.0
I0104 12:45:29.186673   11869 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.k8s.io/etcd:3.5.15-0
I0104 12:45:29.189937   11869 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.k8s.io/pause:3.10
I0104 12:45:29.191311   11869 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.k8s.io/coredns/coredns:v1.11.1
I0104 12:45:29.199878   11869 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.k8s.io/kube-apiserver:v1.31.0
I0104 12:45:29.203705   11869 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.k8s.io/kube-scheduler:v1.31.0
I0104 12:45:29.207851   11869 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.k8s.io/kube-proxy:v1.31.0
I0104 12:45:29.212631   11869 cache_images.go:116] "registry.k8s.io/etcd:3.5.15-0" needs transfer: "registry.k8s.io/etcd:3.5.15-0" does not exist at hash "2e96e5913fc06e3d26915af3d0f2ca5048cc4b6327e661e80da792cbf8d8d9d4" in container runtime
I0104 12:45:29.212650   11869 docker.go:337] Removing image: registry.k8s.io/etcd:3.5.15-0
I0104 12:45:29.212683   11869 ssh_runner.go:195] Run: docker rmi registry.k8s.io/etcd:3.5.15-0
I0104 12:45:29.212803   11869 cache_images.go:116] "registry.k8s.io/kube-controller-manager:v1.31.0" needs transfer: "registry.k8s.io/kube-controller-manager:v1.31.0" does not exist at hash "045733566833c40b15806c9b87d27f08e455e069833752e0e6ad7a76d37cb2b1" in container runtime
I0104 12:45:29.212817   11869 docker.go:337] Removing image: registry.k8s.io/kube-controller-manager:v1.31.0
I0104 12:45:29.212838   11869 ssh_runner.go:195] Run: docker rmi registry.k8s.io/kube-controller-manager:v1.31.0
I0104 12:45:29.214274   11869 cache_images.go:116] "registry.k8s.io/pause:3.10" needs transfer: "registry.k8s.io/pause:3.10" does not exist at hash "873ed75102791e5b0b8a7fcd41606c92fcec98d56d05ead4ac5131650004c136" in container runtime
I0104 12:45:29.214292   11869 docker.go:337] Removing image: registry.k8s.io/pause:3.10
I0104 12:45:29.214320   11869 ssh_runner.go:195] Run: docker rmi registry.k8s.io/pause:3.10
I0104 12:45:29.214365   11869 cache_images.go:116] "registry.k8s.io/coredns/coredns:v1.11.1" needs transfer: "registry.k8s.io/coredns/coredns:v1.11.1" does not exist at hash "cbb01a7bd410dc08ba382018ab909a674fb0e48687f0c00797ed5bc34fcc6bb4" in container runtime
I0104 12:45:29.214376   11869 docker.go:337] Removing image: registry.k8s.io/coredns/coredns:v1.11.1
I0104 12:45:29.214396   11869 ssh_runner.go:195] Run: docker rmi registry.k8s.io/coredns/coredns:v1.11.1
I0104 12:45:29.222947   11869 cache_images.go:116] "registry.k8s.io/kube-apiserver:v1.31.0" needs transfer: "registry.k8s.io/kube-apiserver:v1.31.0" does not exist at hash "604f5db92eaa823d11c141d8825f1460206f6bf29babca2a909a698dc22055d3" in container runtime
I0104 12:45:29.222966   11869 docker.go:337] Removing image: registry.k8s.io/kube-apiserver:v1.31.0
I0104 12:45:29.222996   11869 ssh_runner.go:195] Run: docker rmi registry.k8s.io/kube-apiserver:v1.31.0
I0104 12:45:29.223133   11869 cache_images.go:116] "registry.k8s.io/kube-scheduler:v1.31.0" needs transfer: "registry.k8s.io/kube-scheduler:v1.31.0" does not exist at hash "1766f54c897f0e57040741e6741462f2e3a7d754705f446c9f729c7e1230fb94" in container runtime
I0104 12:45:29.223144   11869 docker.go:337] Removing image: registry.k8s.io/kube-scheduler:v1.31.0
I0104 12:45:29.223168   11869 ssh_runner.go:195] Run: docker rmi registry.k8s.io/kube-scheduler:v1.31.0
I0104 12:45:29.246082   11869 cache_images.go:116] "registry.k8s.io/kube-proxy:v1.31.0" needs transfer: "registry.k8s.io/kube-proxy:v1.31.0" does not exist at hash "ad83b2ca7b09e6162f96f933eecded731cbebf049c78f941fd0ce560a86b6494" in container runtime
I0104 12:45:29.246101   11869 docker.go:337] Removing image: registry.k8s.io/kube-proxy:v1.31.0
I0104 12:45:29.246146   11869 ssh_runner.go:195] Run: docker rmi registry.k8s.io/kube-proxy:v1.31.0
I0104 12:45:29.246192   11869 cache_images.go:289] Loading image from: /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/etcd_3.5.15-0
I0104 12:45:29.246262   11869 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/images/etcd_3.5.15-0
I0104 12:45:29.246308   11869 cache_images.go:289] Loading image from: /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-controller-manager_v1.31.0
I0104 12:45:29.246360   11869 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/images/kube-controller-manager_v1.31.0
I0104 12:45:29.246406   11869 cache_images.go:289] Loading image from: /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/pause_3.10
I0104 12:45:29.246459   11869 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/images/pause_3.10
I0104 12:45:29.253613   11869 cache_images.go:289] Loading image from: /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/coredns/coredns_v1.11.1
I0104 12:45:29.253678   11869 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/images/coredns_v1.11.1
I0104 12:45:29.253720   11869 cache_images.go:289] Loading image from: /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-apiserver_v1.31.0
I0104 12:45:29.253759   11869 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/images/kube-apiserver_v1.31.0
I0104 12:45:29.253797   11869 cache_images.go:289] Loading image from: /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-scheduler_v1.31.0
I0104 12:45:29.253834   11869 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/images/kube-scheduler_v1.31.0
I0104 12:45:29.260190   11869 ssh_runner.go:352] existence check for /var/lib/minikube/images/etcd_3.5.15-0: stat -c "%s %y" /var/lib/minikube/images/etcd_3.5.15-0: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/images/etcd_3.5.15-0': No such file or directory
I0104 12:45:29.260205   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/etcd_3.5.15-0 --> /var/lib/minikube/images/etcd_3.5.15-0 (56918528 bytes)
I0104 12:45:29.260255   11869 cache_images.go:289] Loading image from: /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-proxy_v1.31.0
I0104 12:45:29.260322   11869 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/images/kube-proxy_v1.31.0
I0104 12:45:29.260348   11869 ssh_runner.go:352] existence check for /var/lib/minikube/images/kube-controller-manager_v1.31.0: stat -c "%s %y" /var/lib/minikube/images/kube-controller-manager_v1.31.0: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/images/kube-controller-manager_v1.31.0': No such file or directory
I0104 12:45:29.260354   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-controller-manager_v1.31.0 --> /var/lib/minikube/images/kube-controller-manager_v1.31.0 (26251264 bytes)
I0104 12:45:29.260388   11869 ssh_runner.go:352] existence check for /var/lib/minikube/images/pause_3.10: stat -c "%s %y" /var/lib/minikube/images/pause_3.10: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/images/pause_3.10': No such file or directory
I0104 12:45:29.260397   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/pause_3.10 --> /var/lib/minikube/images/pause_3.10 (321024 bytes)
I0104 12:45:29.260406   11869 ssh_runner.go:352] existence check for /var/lib/minikube/images/kube-apiserver_v1.31.0: stat -c "%s %y" /var/lib/minikube/images/kube-apiserver_v1.31.0: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/images/kube-apiserver_v1.31.0': No such file or directory
I0104 12:45:29.260414   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-apiserver_v1.31.0 --> /var/lib/minikube/images/kube-apiserver_v1.31.0 (28073472 bytes)
I0104 12:45:29.260436   11869 ssh_runner.go:352] existence check for /var/lib/minikube/images/coredns_v1.11.1: stat -c "%s %y" /var/lib/minikube/images/coredns_v1.11.1: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/images/coredns_v1.11.1': No such file or directory
I0104 12:45:29.260442   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/coredns/coredns_v1.11.1 --> /var/lib/minikube/images/coredns_v1.11.1 (18189312 bytes)
I0104 12:45:29.260449   11869 ssh_runner.go:352] existence check for /var/lib/minikube/images/kube-scheduler_v1.31.0: stat -c "%s %y" /var/lib/minikube/images/kube-scheduler_v1.31.0: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/images/kube-scheduler_v1.31.0': No such file or directory
I0104 12:45:29.260455   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-scheduler_v1.31.0 --> /var/lib/minikube/images/kube-scheduler_v1.31.0 (20207104 bytes)
I0104 12:45:29.267349   11869 ssh_runner.go:352] existence check for /var/lib/minikube/images/kube-proxy_v1.31.0: stat -c "%s %y" /var/lib/minikube/images/kube-proxy_v1.31.0: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/images/kube-proxy_v1.31.0': No such file or directory
I0104 12:45:29.267381   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-proxy_v1.31.0 --> /var/lib/minikube/images/kube-proxy_v1.31.0 (30210560 bytes)
I0104 12:45:29.340303   11869 docker.go:304] Loading image: /var/lib/minikube/images/pause_3.10
I0104 12:45:29.340315   11869 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/pause_3.10 | docker load"
I0104 12:45:29.517631   11869 cache_images.go:321] Transferred and loaded /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/pause_3.10 from cache
I0104 12:45:29.517650   11869 docker.go:304] Loading image: /var/lib/minikube/images/coredns_v1.11.1
I0104 12:45:29.517657   11869 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/coredns_v1.11.1 | docker load"
I0104 12:45:30.127385   11869 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} gcr.io/k8s-minikube/storage-provisioner:v5
I0104 12:45:30.393662   11869 cache_images.go:321] Transferred and loaded /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/coredns/coredns_v1.11.1 from cache
I0104 12:45:30.393680   11869 docker.go:304] Loading image: /var/lib/minikube/images/kube-scheduler_v1.31.0
I0104 12:45:30.393688   11869 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-scheduler_v1.31.0 | docker load"
I0104 12:45:30.393700   11869 cache_images.go:116] "gcr.io/k8s-minikube/storage-provisioner:v5" needs transfer: "gcr.io/k8s-minikube/storage-provisioner:v5" does not exist at hash "6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562" in container runtime
I0104 12:45:30.393716   11869 docker.go:337] Removing image: gcr.io/k8s-minikube/storage-provisioner:v5
I0104 12:45:30.393746   11869 ssh_runner.go:195] Run: docker rmi gcr.io/k8s-minikube/storage-provisioner:v5
I0104 12:45:31.215828   11869 cache_images.go:321] Transferred and loaded /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-scheduler_v1.31.0 from cache
I0104 12:45:31.215852   11869 docker.go:304] Loading image: /var/lib/minikube/images/kube-controller-manager_v1.31.0
I0104 12:45:31.215858   11869 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-controller-manager_v1.31.0 | docker load"
I0104 12:45:31.215865   11869 cache_images.go:289] Loading image from: /home/nazirok/.minikube/cache/images/amd64/gcr.io/k8s-minikube/storage-provisioner_v5
I0104 12:45:31.215924   11869 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/images/storage-provisioner_v5
I0104 12:45:31.690100   11869 cache_images.go:321] Transferred and loaded /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-controller-manager_v1.31.0 from cache
I0104 12:45:31.690164   11869 docker.go:304] Loading image: /var/lib/minikube/images/kube-apiserver_v1.31.0
I0104 12:45:31.690168   11869 ssh_runner.go:352] existence check for /var/lib/minikube/images/storage-provisioner_v5: stat -c "%s %y" /var/lib/minikube/images/storage-provisioner_v5: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/images/storage-provisioner_v5': No such file or directory
I0104 12:45:31.690185   11869 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-apiserver_v1.31.0 | docker load"
I0104 12:45:31.690216   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/cache/images/amd64/gcr.io/k8s-minikube/storage-provisioner_v5 --> /var/lib/minikube/images/storage-provisioner_v5 (9060352 bytes)
I0104 12:45:32.240752   11869 cache_images.go:321] Transferred and loaded /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-apiserver_v1.31.0 from cache
I0104 12:45:32.240805   11869 docker.go:304] Loading image: /var/lib/minikube/images/kube-proxy_v1.31.0
I0104 12:45:32.240826   11869 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-proxy_v1.31.0 | docker load"
I0104 12:45:32.980305   11869 cache_images.go:321] Transferred and loaded /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/kube-proxy_v1.31.0 from cache
I0104 12:45:32.980320   11869 docker.go:304] Loading image: /var/lib/minikube/images/etcd_3.5.15-0
I0104 12:45:32.980326   11869 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/etcd_3.5.15-0 | docker load"
I0104 12:45:35.733331   11869 ssh_runner.go:235] Completed: /bin/bash -c "sudo cat /var/lib/minikube/images/etcd_3.5.15-0 | docker load": (2.752968594s)
I0104 12:45:35.733364   11869 cache_images.go:321] Transferred and loaded /home/nazirok/.minikube/cache/images/amd64/registry.k8s.io/etcd_3.5.15-0 from cache
I0104 12:45:35.733404   11869 docker.go:304] Loading image: /var/lib/minikube/images/storage-provisioner_v5
I0104 12:45:35.733421   11869 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/storage-provisioner_v5 | docker load"
I0104 12:45:35.972038   11869 cache_images.go:321] Transferred and loaded /home/nazirok/.minikube/cache/images/amd64/gcr.io/k8s-minikube/storage-provisioner_v5 from cache
I0104 12:45:35.972093   11869 cache_images.go:123] Successfully loaded all cached images
I0104 12:45:35.972103   11869 cache_images.go:92] duration metric: took 7.426233552s to LoadCachedImages
I0104 12:45:35.972122   11869 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I0104 12:45:35.972298   11869 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:localStorageCapacityIsolation Value:false}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0104 12:45:35.972477   11869 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0104 12:45:36.223610   11869 cni.go:84] Creating CNI manager for ""
I0104 12:45:36.223634   11869 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0104 12:45:36.223669   11869 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0104 12:45:36.223704   11869 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth localStorageCapacityIsolation:false runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0104 12:45:36.223908   11869 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
localStorageCapacityIsolation: false
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0104 12:45:36.224011   11869 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I0104 12:45:36.243779   11869 binaries.go:47] Didn't find k8s binaries: sudo ls /var/lib/minikube/binaries/v1.31.0: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/binaries/v1.31.0': No such file or directory

Initiating transfer...
I0104 12:45:36.243866   11869 ssh_runner.go:195] Run: sudo mkdir -p /var/lib/minikube/binaries/v1.31.0
I0104 12:45:36.261363   11869 binary.go:74] Not caching binary, using https://dl.k8s.io/release/v1.31.0/bin/linux/amd64/kubectl?checksum=file:https://dl.k8s.io/release/v1.31.0/bin/linux/amd64/kubectl.sha256
I0104 12:45:36.261376   11869 binary.go:74] Not caching binary, using https://dl.k8s.io/release/v1.31.0/bin/linux/amd64/kubelet?checksum=file:https://dl.k8s.io/release/v1.31.0/bin/linux/amd64/kubelet.sha256
I0104 12:45:36.261407   11869 binary.go:74] Not caching binary, using https://dl.k8s.io/release/v1.31.0/bin/linux/amd64/kubeadm?checksum=file:https://dl.k8s.io/release/v1.31.0/bin/linux/amd64/kubeadm.sha256
I0104 12:45:36.261426   11869 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0104 12:45:36.261457   11869 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/binaries/v1.31.0/kubectl
I0104 12:45:36.261497   11869 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/binaries/v1.31.0/kubeadm
I0104 12:45:36.266036   11869 ssh_runner.go:352] existence check for /var/lib/minikube/binaries/v1.31.0/kubeadm: stat -c "%s %y" /var/lib/minikube/binaries/v1.31.0/kubeadm: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/binaries/v1.31.0/kubeadm': No such file or directory
I0104 12:45:36.266055   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/cache/linux/amd64/v1.31.0/kubeadm --> /var/lib/minikube/binaries/v1.31.0/kubeadm (58290328 bytes)
I0104 12:45:36.280969   11869 ssh_runner.go:352] existence check for /var/lib/minikube/binaries/v1.31.0/kubectl: stat -c "%s %y" /var/lib/minikube/binaries/v1.31.0/kubectl: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/binaries/v1.31.0/kubectl': No such file or directory
I0104 12:45:36.280982   11869 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/binaries/v1.31.0/kubelet
I0104 12:45:36.280982   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/cache/linux/amd64/v1.31.0/kubectl --> /var/lib/minikube/binaries/v1.31.0/kubectl (56381592 bytes)
I0104 12:45:36.292577   11869 ssh_runner.go:352] existence check for /var/lib/minikube/binaries/v1.31.0/kubelet: stat -c "%s %y" /var/lib/minikube/binaries/v1.31.0/kubelet: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/binaries/v1.31.0/kubelet': No such file or directory
I0104 12:45:36.292593   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/cache/linux/amd64/v1.31.0/kubelet --> /var/lib/minikube/binaries/v1.31.0/kubelet (76865848 bytes)
I0104 12:45:36.711336   11869 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0104 12:45:36.718584   11869 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0104 12:45:36.732375   11869 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0104 12:45:36.746095   11869 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2186 bytes)
I0104 12:45:36.759760   11869 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0104 12:45:36.762243   11869 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0104 12:45:36.770498   11869 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 12:45:36.861011   11869 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0104 12:45:36.871670   11869 certs.go:68] Setting up /home/nazirok/.minikube/profiles/minikube for IP: 192.168.49.2
I0104 12:45:36.871677   11869 certs.go:194] generating shared ca certs ...
I0104 12:45:36.871686   11869 certs.go:226] acquiring lock for ca certs: {Name:mk0bbe120428118807d8d67db1446ca6a98e9b41 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0104 12:45:36.871835   11869 certs.go:235] skipping valid "minikubeCA" ca cert: /home/nazirok/.minikube/ca.key
I0104 12:45:36.871887   11869 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/nazirok/.minikube/proxy-client-ca.key
I0104 12:45:36.871893   11869 certs.go:256] generating profile certs ...
I0104 12:45:36.871962   11869 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/nazirok/.minikube/profiles/minikube/client.key
I0104 12:45:36.872014   11869 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/nazirok/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0104 12:45:36.872359   11869 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/nazirok/.minikube/profiles/minikube/proxy-client.key
I0104 12:45:36.872510   11869 certs.go:484] found cert: /home/nazirok/.minikube/certs/ca-key.pem (1679 bytes)
I0104 12:45:36.872532   11869 certs.go:484] found cert: /home/nazirok/.minikube/certs/ca.pem (1082 bytes)
I0104 12:45:36.872551   11869 certs.go:484] found cert: /home/nazirok/.minikube/certs/cert.pem (1123 bytes)
I0104 12:45:36.872568   11869 certs.go:484] found cert: /home/nazirok/.minikube/certs/key.pem (1679 bytes)
I0104 12:45:36.872954   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0104 12:45:36.893224   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0104 12:45:36.912731   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0104 12:45:36.931267   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0104 12:45:36.949698   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0104 12:45:36.968109   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0104 12:45:36.986568   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0104 12:45:37.005205   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0104 12:45:37.023589   11869 ssh_runner.go:362] scp /home/nazirok/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0104 12:45:37.042214   11869 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0104 12:45:37.055879   11869 ssh_runner.go:195] Run: openssl version
I0104 12:45:37.062479   11869 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0104 12:45:37.070347   11869 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0104 12:45:37.073047   11869 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov  2 17:39 /usr/share/ca-certificates/minikubeCA.pem
I0104 12:45:37.073082   11869 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0104 12:45:37.078429   11869 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0104 12:45:37.085728   11869 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0104 12:45:37.088124   11869 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0104 12:45:37.088150   11869 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:8000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:localStorageCapacityIsolation Value:false}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/nazirok:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0104 12:45:37.088224   11869 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0104 12:45:37.101516   11869 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0104 12:45:37.108163   11869 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0104 12:45:37.114887   11869 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0104 12:45:37.114920   11869 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0104 12:45:37.121738   11869 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0104 12:45:37.121745   11869 kubeadm.go:157] found existing configuration files:

I0104 12:45:37.121778   11869 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0104 12:45:37.128523   11869 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0104 12:45:37.128556   11869 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0104 12:45:37.135150   11869 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0104 12:45:37.141852   11869 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0104 12:45:37.141888   11869 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0104 12:45:37.148272   11869 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0104 12:45:37.154909   11869 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0104 12:45:37.154943   11869 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0104 12:45:37.161513   11869 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0104 12:45:37.168153   11869 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0104 12:45:37.168187   11869 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0104 12:45:37.174651   11869 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0104 12:45:37.199907   11869 kubeadm.go:310] W0104 09:45:37.199411    2370 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I0104 12:45:37.200198   11869 kubeadm.go:310] W0104 09:45:37.199851    2370 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "InitConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I0104 12:45:37.213186   11869 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0104 12:45:37.215985   11869 kubeadm.go:310] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/5.15.0-53-generic\n", err: exit status 1
I0104 12:45:37.254771   11869 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0104 12:45:47.284649   11869 kubeadm.go:310] [init] Using Kubernetes version: v1.31.0
I0104 12:45:47.284882   11869 kubeadm.go:310] [preflight] Running pre-flight checks
I0104 12:45:47.285205   11869 kubeadm.go:310] [preflight] The system verification failed. Printing the output from the verification:
I0104 12:45:47.285393   11869 kubeadm.go:310] [0;37mKERNEL_VERSION[0m: [0;32m5.15.0-53-generic[0m
I0104 12:45:47.285502   11869 kubeadm.go:310] [0;37mOS[0m: [0;32mLinux[0m
I0104 12:45:47.285646   11869 kubeadm.go:310] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I0104 12:45:47.285797   11869 kubeadm.go:310] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I0104 12:45:47.285963   11869 kubeadm.go:310] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I0104 12:45:47.286120   11869 kubeadm.go:310] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I0104 12:45:47.286274   11869 kubeadm.go:310] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I0104 12:45:47.286431   11869 kubeadm.go:310] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I0104 12:45:47.286588   11869 kubeadm.go:310] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I0104 12:45:47.286729   11869 kubeadm.go:310] [0;37mCGROUPS_IO[0m: [0;32menabled[0m
I0104 12:45:47.286966   11869 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0104 12:45:47.287278   11869 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0104 12:45:47.287574   11869 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0104 12:45:47.287788   11869 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0104 12:45:47.290118   11869 out.go:235]     ‚ñ™ Generating certificates and keys ...
I0104 12:45:47.290493   11869 kubeadm.go:310] [certs] Using existing ca certificate authority
I0104 12:45:47.290790   11869 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0104 12:45:47.291027   11869 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0104 12:45:47.291245   11869 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0104 12:45:47.291544   11869 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0104 12:45:47.291763   11869 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0104 12:45:47.291963   11869 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0104 12:45:47.292350   11869 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0104 12:45:47.292631   11869 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0104 12:45:47.293027   11869 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0104 12:45:47.293290   11869 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0104 12:45:47.293558   11869 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0104 12:45:47.293723   11869 kubeadm.go:310] [certs] Generating "sa" key and public key
I0104 12:45:47.293914   11869 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0104 12:45:47.294078   11869 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0104 12:45:47.294317   11869 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0104 12:45:47.294509   11869 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0104 12:45:47.294764   11869 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0104 12:45:47.294974   11869 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0104 12:45:47.295303   11869 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0104 12:45:47.295544   11869 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0104 12:45:47.297902   11869 out.go:235]     ‚ñ™ Booting up control plane ...
I0104 12:45:47.298367   11869 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0104 12:45:47.298629   11869 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0104 12:45:47.298907   11869 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0104 12:45:47.299297   11869 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0104 12:45:47.299598   11869 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0104 12:45:47.299726   11869 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0104 12:45:47.300241   11869 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0104 12:45:47.300615   11869 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0104 12:45:47.300807   11869 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.001836824s
I0104 12:45:47.301053   11869 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0104 12:45:47.301267   11869 kubeadm.go:310] [api-check] The API server is healthy after 5.503184075s
I0104 12:45:47.301674   11869 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0104 12:45:47.302124   11869 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0104 12:45:47.302319   11869 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0104 12:45:47.302894   11869 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0104 12:45:47.303075   11869 kubeadm.go:310] [bootstrap-token] Using token: g5locw.vpqgng1ixh7jzrno
I0104 12:45:47.306412   11869 out.go:235]     ‚ñ™ Configuring RBAC rules ...
I0104 12:45:47.306938   11869 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0104 12:45:47.307274   11869 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0104 12:45:47.307740   11869 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0104 12:45:47.308168   11869 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0104 12:45:47.308727   11869 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0104 12:45:47.309124   11869 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0104 12:45:47.309568   11869 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0104 12:45:47.309768   11869 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0104 12:45:47.309954   11869 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0104 12:45:47.309980   11869 kubeadm.go:310] 
I0104 12:45:47.310206   11869 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0104 12:45:47.310223   11869 kubeadm.go:310] 
I0104 12:45:47.310510   11869 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0104 12:45:47.310521   11869 kubeadm.go:310] 
I0104 12:45:47.310618   11869 kubeadm.go:310]   mkdir -p $HOME/.kube
I0104 12:45:47.310834   11869 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0104 12:45:47.311037   11869 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0104 12:45:47.311048   11869 kubeadm.go:310] 
I0104 12:45:47.311268   11869 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0104 12:45:47.311280   11869 kubeadm.go:310] 
I0104 12:45:47.311434   11869 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0104 12:45:47.311443   11869 kubeadm.go:310] 
I0104 12:45:47.311612   11869 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0104 12:45:47.311855   11869 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0104 12:45:47.312077   11869 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0104 12:45:47.312088   11869 kubeadm.go:310] 
I0104 12:45:47.312361   11869 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0104 12:45:47.312696   11869 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0104 12:45:47.312707   11869 kubeadm.go:310] 
I0104 12:45:47.313024   11869 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token g5locw.vpqgng1ixh7jzrno \
I0104 12:45:47.313402   11869 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:ad5fafdeca890a0668c53b89ccd4c1ed9eb3b2b9c1ec60199ccb217acb8effec \
I0104 12:45:47.313476   11869 kubeadm.go:310] 	--control-plane 
I0104 12:45:47.313486   11869 kubeadm.go:310] 
I0104 12:45:47.313791   11869 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0104 12:45:47.313802   11869 kubeadm.go:310] 
I0104 12:45:47.314096   11869 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token g5locw.vpqgng1ixh7jzrno \
I0104 12:45:47.314485   11869 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:ad5fafdeca890a0668c53b89ccd4c1ed9eb3b2b9c1ec60199ccb217acb8effec 
I0104 12:45:47.314501   11869 cni.go:84] Creating CNI manager for ""
I0104 12:45:47.314529   11869 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0104 12:45:47.316874   11869 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0104 12:45:47.320922   11869 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0104 12:45:47.349829   11869 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0104 12:45:47.409436   11869 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0104 12:45:47.409529   11869 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0104 12:45:47.409592   11869 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_01_04T12_45_47_0700 minikube.k8s.io/version=v1.34.0 minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0104 12:45:47.437702   11869 ops.go:34] apiserver oom_adj: -16
I0104 12:45:47.609817   11869 kubeadm.go:1113] duration metric: took 200.426081ms to wait for elevateKubeSystemPrivileges
I0104 12:45:47.609836   11869 kubeadm.go:394] duration metric: took 10.521685168s to StartCluster
I0104 12:45:47.609852   11869 settings.go:142] acquiring lock: {Name:mkc24b7de80f84e4a3495f34ad7349ef6f4d112e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0104 12:45:47.610045   11869 settings.go:150] Updating kubeconfig:  /home/nazirok/.kube/config
I0104 12:45:47.610887   11869 lock.go:35] WriteFile acquiring /home/nazirok/.kube/config: {Name:mk0d277b08db1d23eedd6b446805a7a5893a4ac4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0104 12:45:47.611204   11869 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0104 12:45:47.611314   11869 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0104 12:45:47.611408   11869 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0104 12:45:47.611435   11869 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0104 12:45:47.611443   11869 addons.go:243] addon storage-provisioner should already be in state true
I0104 12:45:47.611472   11869 host.go:66] Checking if "minikube" exists ...
I0104 12:45:47.611472   11869 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0104 12:45:47.611527   11869 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0104 12:45:47.611554   11869 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0104 12:45:47.611609   11869 addons.go:69] Setting metrics-server=true in profile "minikube"
I0104 12:45:47.611634   11869 addons.go:234] Setting addon metrics-server=true in "minikube"
W0104 12:45:47.611642   11869 addons.go:243] addon metrics-server should already be in state true
I0104 12:45:47.611667   11869 host.go:66] Checking if "minikube" exists ...
I0104 12:45:47.611882   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0104 12:45:47.612095   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0104 12:45:47.612210   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0104 12:45:47.613910   11869 out.go:177] üîé  –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Kubernetes –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è ...
I0104 12:45:47.617815   11869 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 12:45:47.661966   11869 out.go:177]     ‚ñ™ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—Ä–∞–∑ gcr.io/k8s-minikube/storage-provisioner:v5
I0104 12:45:47.667248   11869 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0104 12:45:47.667263   11869 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0104 12:45:47.667340   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:47.677477   11869 out.go:177]     ‚ñ™ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—Ä–∞–∑ registry.k8s.io/metrics-server/metrics-server:v0.7.2
I0104 12:45:47.682478   11869 addons.go:431] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I0104 12:45:47.682494   11869 ssh_runner.go:362] scp metrics-server/metrics-apiservice.yaml --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I0104 12:45:47.682572   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:47.686633   11869 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0104 12:45:47.686646   11869 addons.go:243] addon default-storageclass should already be in state true
I0104 12:45:47.686675   11869 host.go:66] Checking if "minikube" exists ...
I0104 12:45:47.687235   11869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0104 12:45:47.726119   11869 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I0104 12:45:47.726127   11869 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0104 12:45:47.726169   11869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0104 12:45:47.729652   11869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nazirok/.minikube/machines/minikube/id_rsa Username:docker}
I0104 12:45:47.734124   11869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nazirok/.minikube/machines/minikube/id_rsa Username:docker}
I0104 12:45:47.756082   11869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/nazirok/.minikube/machines/minikube/id_rsa Username:docker}
I0104 12:45:47.790514   11869 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0104 12:45:47.802769   11869 api_server.go:52] waiting for apiserver process to appear ...
I0104 12:45:47.802805   11869 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0104 12:45:47.811621   11869 api_server.go:72] duration metric: took 200.393077ms to wait for apiserver process to appear ...
I0104 12:45:47.811632   11869 api_server.go:88] waiting for apiserver healthz status ...
I0104 12:45:47.811642   11869 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0104 12:45:47.814517   11869 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0104 12:45:47.819065   11869 api_server.go:141] control plane version: v1.31.0
I0104 12:45:47.819077   11869 api_server.go:131] duration metric: took 7.441435ms to wait for apiserver health ...
I0104 12:45:47.819081   11869 system_pods.go:43] waiting for kube-system pods to appear ...
I0104 12:45:47.823586   11869 system_pods.go:59] 4 kube-system pods found
I0104 12:45:47.823597   11869 system_pods.go:61] "etcd-minikube" [515f5086-8d5b-4205-9093-6c675763a408] Pending
I0104 12:45:47.823600   11869 system_pods.go:61] "kube-apiserver-minikube" [8e4ec7d0-9af7-4091-b754-05f40faf2437] Pending
I0104 12:45:47.823602   11869 system_pods.go:61] "kube-controller-manager-minikube" [560692ae-0c57-4283-99d4-d8ea6697174d] Pending
I0104 12:45:47.823604   11869 system_pods.go:61] "kube-scheduler-minikube" [27b94dde-94b2-4eee-a974-3e788e739b72] Pending
I0104 12:45:47.823607   11869 system_pods.go:74] duration metric: took 4.523154ms to wait for pod list to return data ...
I0104 12:45:47.823614   11869 kubeadm.go:582] duration metric: took 212.386969ms to wait for: map[apiserver:true system_pods:true]
I0104 12:45:47.823620   11869 node_conditions.go:102] verifying NodePressure condition ...
I0104 12:45:47.825906   11869 node_conditions.go:122] node storage ephemeral capacity is 0
I0104 12:45:47.825917   11869 node_conditions.go:123] node cpu capacity is 4
I0104 12:45:47.825923   11869 node_conditions.go:105] duration metric: took 2.300561ms to run NodePressure ...
I0104 12:45:47.825931   11869 start.go:241] waiting for startup goroutines ...
I0104 12:45:47.830397   11869 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0104 12:45:47.858027   11869 addons.go:431] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I0104 12:45:47.858035   11869 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I0104 12:45:47.862319   11869 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0104 12:45:47.876268   11869 addons.go:431] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I0104 12:45:47.876276   11869 ssh_runner.go:362] scp metrics-server/metrics-server-rbac.yaml --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I0104 12:45:47.904110   11869 addons.go:431] installing /etc/kubernetes/addons/metrics-server-service.yaml
I0104 12:45:47.904128   11869 ssh_runner.go:362] scp metrics-server/metrics-server-service.yaml --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I0104 12:45:47.920788   11869 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0104 12:45:48.336764   11869 addons.go:475] Verifying addon metrics-server=true in "minikube"
I0104 12:45:48.339127   11869 out.go:177] üåü  –í–∫–ª—é—á–µ–Ω–Ω—ã–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è: storage-provisioner, default-storageclass, metrics-server
I0104 12:45:48.342905   11869 addons.go:510] duration metric: took 731.594749ms for enable addons: enabled=[storage-provisioner default-storageclass metrics-server]
I0104 12:45:48.342952   11869 start.go:246] waiting for cluster config update ...
I0104 12:45:48.342973   11869 start.go:255] writing updated cluster config ...
I0104 12:45:48.343396   11869 ssh_runner.go:195] Run: rm -f paused
I0104 12:45:48.353268   11869 out.go:177] üí°  kubectl not found. If you need it, try: 'minikube kubectl -- get pods -A'
I0104 12:45:48.355495   11869 out.go:177] üèÑ  –ì–æ—Ç–æ–≤–æ! kubectl –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞ "minikube" –∏ "default" –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∏–º—ë–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é


==> Docker <==
Jan 04 09:45:28 minikube cri-dockerd[1549]: time="2025-01-04T09:45:28Z" level=info msg="Start docker client with request timeout 0s"
Jan 04 09:45:28 minikube cri-dockerd[1549]: time="2025-01-04T09:45:28Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jan 04 09:45:28 minikube cri-dockerd[1549]: time="2025-01-04T09:45:28Z" level=info msg="Loaded network plugin cni"
Jan 04 09:45:28 minikube cri-dockerd[1549]: time="2025-01-04T09:45:28Z" level=info msg="Docker cri networking managed by network plugin cni"
Jan 04 09:45:28 minikube cri-dockerd[1549]: time="2025-01-04T09:45:28Z" level=info msg="Setting cgroupDriver systemd"
Jan 04 09:45:28 minikube cri-dockerd[1549]: time="2025-01-04T09:45:28Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jan 04 09:45:28 minikube cri-dockerd[1549]: time="2025-01-04T09:45:28Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jan 04 09:45:28 minikube cri-dockerd[1549]: time="2025-01-04T09:45:28Z" level=info msg="Start cri-dockerd grpc backend"
Jan 04 09:45:28 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jan 04 09:45:41 minikube cri-dockerd[1549]: time="2025-01-04T09:45:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/80bf3f8ca2d91a03eeb1af21e11f54437cb5b2cfb3d1a9a92b6706d7db747dd8/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jan 04 09:45:41 minikube cri-dockerd[1549]: time="2025-01-04T09:45:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6bda717534380e8f3ecddd6bbb8d6b3257b2a39f12121f12f637ba1b0fb90b6d/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jan 04 09:45:41 minikube cri-dockerd[1549]: time="2025-01-04T09:45:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/315851a16e21b41414656b512289d2e7a961b074a95def9e8d343570fdf765b5/resolv.conf as [nameserver 192.168.49.1 options trust-ad ndots:0 edns0]"
Jan 04 09:45:41 minikube cri-dockerd[1549]: time="2025-01-04T09:45:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5b841676aa07282dffe2b1bf0b0f0bb2214b692245472ba41cd724ecef73ba41/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jan 04 09:45:52 minikube cri-dockerd[1549]: time="2025-01-04T09:45:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e522c6b00ec77c4e150a0b1d0e5891dfc4b03edcd2737609fc1c9153e92cab01/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jan 04 09:45:52 minikube cri-dockerd[1549]: time="2025-01-04T09:45:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d0b2e77854afc5f5aa1ad9337490489b0a18a22c27fc6fd164075063350f54eb/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jan 04 09:45:52 minikube cri-dockerd[1549]: time="2025-01-04T09:45:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/17bac55f67fa229b86063f60c79c0dbd2a4186cecaecb3cb3f997f05e11909e2/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 04 09:45:52 minikube cri-dockerd[1549]: time="2025-01-04T09:45:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/879eaeab05eb7481996516cac0ecd75a1f1bb09bd46fc212a8ada93a24a2ef14/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jan 04 09:45:53 minikube dockerd[1288]: time="2025-01-04T09:45:53.138226902Z" level=warning msg="reference for unknown type: " digest="sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9" remote="registry.k8s.io/metrics-server/metrics-server@sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9"
Jan 04 09:45:53 minikube cri-dockerd[1549]: time="2025-01-04T09:45:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b2328946a311385ee571c530afc579a878bc8f30c7f6a0ae893f6c6d1c611502/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jan 04 09:45:57 minikube cri-dockerd[1549]: time="2025-01-04T09:45:57Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jan 04 09:45:59 minikube cri-dockerd[1549]: time="2025-01-04T09:45:59Z" level=info msg="Stop pulling image registry.k8s.io/metrics-server/metrics-server:v0.7.2@sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9: Status: Downloaded newer image for registry.k8s.io/metrics-server/metrics-server@sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9"
Jan 04 10:00:19 minikube cri-dockerd[1549]: time="2025-01-04T10:00:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/edc5781bb95997aad6a0f71e8750163e0e778f0a1f73ba6a76e386cafcf64169/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 04 10:00:23 minikube cri-dockerd[1549]: time="2025-01-04T10:00:23Z" level=info msg="Stop pulling image shestera/scaletestapp:latest: Status: Downloaded newer image for shestera/scaletestapp:latest"
Jan 04 10:09:16 minikube cri-dockerd[1549]: time="2025-01-04T10:09:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b659d8ae00b95611a6538e1f137243bd50372fa38d2a1f6bc744b147d3bb90b0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 04 10:09:16 minikube cri-dockerd[1549]: time="2025-01-04T10:09:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/acfe04e896227a9cc4c40e97c1d2300100e8b2877f358c3674b7a713e6134695/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 04 10:09:16 minikube cri-dockerd[1549]: time="2025-01-04T10:09:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/81c6c2cd5451dede6a1a3ed5652b0660a2960ec5f7da0564bfe3327d52e759f8/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 04 10:09:16 minikube cri-dockerd[1549]: time="2025-01-04T10:09:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/99cfad83511c9b5a6c37ba991441cbd3c712df360a9d0c191720bad0e34a27a9/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 04 10:09:17 minikube cri-dockerd[1549]: time="2025-01-04T10:09:17Z" level=info msg="Stop pulling image shestera/scaletestapp:latest: Status: Image is up to date for shestera/scaletestapp:latest"
Jan 04 10:09:18 minikube cri-dockerd[1549]: time="2025-01-04T10:09:18Z" level=info msg="Stop pulling image shestera/scaletestapp:latest: Status: Image is up to date for shestera/scaletestapp:latest"
Jan 04 10:09:20 minikube cri-dockerd[1549]: time="2025-01-04T10:09:20Z" level=info msg="Stop pulling image shestera/scaletestapp:latest: Status: Image is up to date for shestera/scaletestapp:latest"
Jan 04 10:09:21 minikube cri-dockerd[1549]: time="2025-01-04T10:09:21Z" level=info msg="Stop pulling image shestera/scaletestapp:latest: Status: Image is up to date for shestera/scaletestapp:latest"
Jan 04 10:13:12 minikube dockerd[1288]: time="2025-01-04T10:13:12.668786280Z" level=info msg="ignoring event" container=6eeae40b24a8e3b2f12aae134407bdec32c509c2faa7880ff834f0d1e56bdf5e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 10:13:12 minikube dockerd[1288]: time="2025-01-04T10:13:12.694499631Z" level=info msg="ignoring event" container=e842e0c688408bf58eb8b32754b64cedaf941c71d1204657cef7b13e4f806c55 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 10:13:12 minikube dockerd[1288]: time="2025-01-04T10:13:12.702576902Z" level=info msg="ignoring event" container=228d8b1c2e9ccf8b7941cc9e738958c1a0dd982e149ccddf1529591de7ea324c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 10:13:12 minikube dockerd[1288]: time="2025-01-04T10:13:12.767605092Z" level=info msg="ignoring event" container=47d6b91328e337611e6f61714cb7648cb6b53ec6a4ab21b511f102c92362b55a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 10:13:12 minikube dockerd[1288]: time="2025-01-04T10:13:12.998786755Z" level=info msg="ignoring event" container=99cfad83511c9b5a6c37ba991441cbd3c712df360a9d0c191720bad0e34a27a9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 10:13:13 minikube dockerd[1288]: time="2025-01-04T10:13:13.082627287Z" level=info msg="ignoring event" container=81c6c2cd5451dede6a1a3ed5652b0660a2960ec5f7da0564bfe3327d52e759f8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 10:13:13 minikube dockerd[1288]: time="2025-01-04T10:13:13.086579833Z" level=info msg="ignoring event" container=acfe04e896227a9cc4c40e97c1d2300100e8b2877f358c3674b7a713e6134695 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 10:13:13 minikube dockerd[1288]: time="2025-01-04T10:13:13.104908131Z" level=info msg="ignoring event" container=b659d8ae00b95611a6538e1f137243bd50372fa38d2a1f6bc744b147d3bb90b0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 10:13:25 minikube dockerd[1288]: time="2025-01-04T10:13:25.683324630Z" level=info msg="ignoring event" container=a3dcca151dca4072982363d4a3fa84201b1b58fef271bc67cc5222999964a4ee module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 10:13:25 minikube dockerd[1288]: time="2025-01-04T10:13:25.961279858Z" level=info msg="ignoring event" container=edc5781bb95997aad6a0f71e8750163e0e778f0a1f73ba6a76e386cafcf64169 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 10:39:40 minikube cri-dockerd[1549]: time="2025-01-04T10:39:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d9869d57f5e789fc6e9d348fb4707e4e6737cb93e02eb0bb7eb7d3556c3a96ec/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 04 11:52:22 minikube dockerd[1288]: time="2025-01-04T11:52:22.160193329Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 04 11:52:22 minikube dockerd[1288]: time="2025-01-04T11:52:22.160553925Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 04 11:52:22 minikube dockerd[1288]: time="2025-01-04T11:52:22.165857906Z" level=error msg="Error running exec 37f8f26a42fcb76096c400b4c56f398cb6dc87ea50eb296a798c854073b9f8e6 in container: OCI runtime exec failed: exec failed: unable to start container process: exec: \"printenv\": executable file not found in $PATH: unknown"
Jan 04 11:52:46 minikube dockerd[1288]: time="2025-01-04T11:52:46.902185985Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 04 11:52:46 minikube dockerd[1288]: time="2025-01-04T11:52:46.902214132Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 04 11:52:46 minikube dockerd[1288]: time="2025-01-04T11:52:46.912742586Z" level=error msg="Error running exec b27d995d66ac8d45e8c3ea6cae0d74c1100ab0417469de8dcd67b4f1f5f8eaaa in container: OCI runtime exec failed: exec failed: unable to start container process: exec: \"printenv\": executable file not found in $PATH: unknown"
Jan 04 14:24:32 minikube cri-dockerd[1549]: time="2025-01-04T14:24:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3cab177f8a378f5d075c752627497b4c0839a0717efe79f93e0beb1d42949f72/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 04 14:24:32 minikube cri-dockerd[1549]: time="2025-01-04T14:24:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/94eacad05d75b7e6059b39bfe00c39d26cef469e91222b7ae9bd6dee962a84b2/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 04 14:24:32 minikube dockerd[1288]: time="2025-01-04T14:24:32.930274465Z" level=warning msg="reference for unknown type: " digest="sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" remote="docker.io/kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Jan 04 14:24:37 minikube cri-dockerd[1549]: time="2025-01-04T14:24:37Z" level=info msg="Stop pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: Status: Downloaded newer image for kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Jan 04 14:24:38 minikube dockerd[1288]: time="2025-01-04T14:24:38.185141122Z" level=warning msg="reference for unknown type: " digest="sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93" remote="docker.io/kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Jan 04 14:24:49 minikube cri-dockerd[1549]: time="2025-01-04T14:24:49Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Extracting [==============================================>    ]  70.75MB/75.78MB"
Jan 04 14:24:49 minikube cri-dockerd[1549]: time="2025-01-04T14:24:49Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: Status: Downloaded newer image for kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Jan 04 14:55:17 minikube dockerd[1288]: time="2025-01-04T14:55:17.902993521Z" level=info msg="ignoring event" container=e28d626b05b85b5305322d030c364e87e47992d349d16f3ae6539516f0ff3d8d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 14:55:33 minikube dockerd[1288]: time="2025-01-04T14:55:33.131486862Z" level=info msg="ignoring event" container=753c6b85ab321c8b2a2f3a90597b088d7d9e2a9c7f521905ba7c4c6fdce2bf42 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 14:55:55 minikube dockerd[1288]: time="2025-01-04T14:55:55.960074910Z" level=info msg="ignoring event" container=46ae829ba739571c5f913a1867ca1ef3dd45490055b139c75251bfb791f3b23f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 14:56:31 minikube dockerd[1288]: time="2025-01-04T14:56:31.903272500Z" level=info msg="ignoring event" container=f7ec1d9dd88e7515eb90d56a2381f7d3e3bbb4de0d36976cdc846ca334cabeca module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 15:00:04 minikube cri-dockerd[1549]: time="2025-01-04T15:00:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/56f7e18f7b2c27d5e1b5e3b0af9cfa2e4ad84dcdd55523faacd7de06c7753f54/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"


==> container status <==
CONTAINER           IMAGE                                                                                                                   CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
0deec14d94089       7641ebbc7239a                                                                                                           3 minutes ago       Running             testapp                     0                   56f7e18f7b2c2       testapp-deployment-8585d4875f-wtt96
e656710920e9b       7641ebbc7239a                                                                                                           6 minutes ago       Running             testapp                     4                   d9869d57f5e78       testapp-deployment-8585d4875f-2mwdd
f7ec1d9dd88e7       7641ebbc7239a                                                                                                           7 minutes ago       Exited              testapp                     3                   d9869d57f5e78       testapp-deployment-8585d4875f-2mwdd
f84386dafd39b       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93                          39 minutes ago      Running             kubernetes-dashboard        0                   94eacad05d75b       kubernetes-dashboard-695b96c756-dgv9h
2c53ca8073782       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c                    39 minutes ago      Running             dashboard-metrics-scraper   0                   3cab177f8a378       dashboard-metrics-scraper-c5db448b4-9x5c7
16528572d8eeb       registry.k8s.io/metrics-server/metrics-server@sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9   5 hours ago         Running             metrics-server              0                   17bac55f67fa2       metrics-server-84c5f94fbc-xrv6m
43211388ebb15       cbb01a7bd410d                                                                                                           5 hours ago         Running             coredns                     0                   d0b2e77854afc       coredns-6f6b679f8f-k69pr
47f532f1abd47       6e38f40d628db                                                                                                           5 hours ago         Running             storage-provisioner         0                   b2328946a3113       storage-provisioner
e15dd2966c45d       cbb01a7bd410d                                                                                                           5 hours ago         Running             coredns                     0                   879eaeab05eb7       coredns-6f6b679f8f-qk6qf
3180d4c0f6706       ad83b2ca7b09e                                                                                                           5 hours ago         Running             kube-proxy                  0                   e522c6b00ec77       kube-proxy-4bmgw
ce20a7eabbeb8       604f5db92eaa8                                                                                                           5 hours ago         Running             kube-apiserver              0                   5b841676aa072       kube-apiserver-minikube
fdb36f89f6788       2e96e5913fc06                                                                                                           5 hours ago         Running             etcd                        0                   315851a16e21b       etcd-minikube
eb8e3bbc0c414       045733566833c                                                                                                           5 hours ago         Running             kube-controller-manager     0                   6bda717534380       kube-controller-manager-minikube
b3cb1c12d6651       1766f54c897f0                                                                                                           5 hours ago         Running             kube-scheduler              0                   80bf3f8ca2d91       kube-scheduler-minikube


==> coredns [43211388ebb1] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[512740297]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jan-2025 09:45:53.313) (total time: 30001ms):
Trace[512740297]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (09:46:23.314)
Trace[512740297]: [30.001101242s] [30.001101242s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[592108132]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jan-2025 09:45:53.313) (total time: 30001ms):
Trace[592108132]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (09:46:23.314)
Trace[592108132]: [30.001477178s] [30.001477178s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[49650409]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jan-2025 09:45:53.313) (total time: 30001ms):
Trace[49650409]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (09:46:23.314)
Trace[49650409]: [30.001764206s] [30.001764206s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> coredns [e15dd2966c45] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[867317962]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jan-2025 09:45:53.294) (total time: 30001ms):
Trace[867317962]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (09:46:23.295)
Trace[867317962]: [30.00136853s] [30.00136853s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[755412514]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jan-2025 09:45:53.293) (total time: 30002ms):
Trace[755412514]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (09:46:23.295)
Trace[755412514]: [30.002087893s] [30.002087893s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1973867719]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jan-2025 09:45:53.294) (total time: 30001ms):
Trace[1973867719]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (09:46:23.295)
Trace[1973867719]: [30.001647411s] [30.001647411s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_01_04T12_45_47_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 04 Jan 2025 09:45:43 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 04 Jan 2025 15:03:55 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 04 Jan 2025 15:00:42 +0000   Sat, 04 Jan 2025 09:45:43 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 04 Jan 2025 15:00:42 +0000   Sat, 04 Jan 2025 09:45:43 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 04 Jan 2025 15:00:42 +0000   Sat, 04 Jan 2025 09:45:43 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 04 Jan 2025 15:00:42 +0000   Sat, 04 Jan 2025 09:45:44 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:            4
  hugepages-1Gi:  0
  hugepages-2Mi:  0
  memory:         32825092Ki
  pods:           110
Allocatable:
  cpu:            4
  hugepages-1Gi:  0
  hugepages-2Mi:  0
  memory:         32825092Ki
  pods:           110
System Info:
  Machine ID:                 207bb637ea7e436a9934b1d4d9ec90ef
  System UUID:                6ed98b66-89ed-47d9-b3a2-bdc82b843c90
  Boot ID:                    1d6b49bd-4ac0-4b8f-a13c-19269c17a1eb
  Kernel Version:             5.15.0-53-generic
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     testapp-deployment-8585d4875f-2mwdd          1 (25%)       1 (25%)     30Mi (0%)        30Mi (0%)      4h24m
  default                     testapp-deployment-8585d4875f-wtt96          1 (25%)       1 (25%)     30Mi (0%)        30Mi (0%)      3m58s
  kube-system                 coredns-6f6b679f8f-k69pr                     100m (2%)     0 (0%)      70Mi (0%)        170Mi (0%)     5h18m
  kube-system                 coredns-6f6b679f8f-qk6qf                     100m (2%)     0 (0%)      70Mi (0%)        170Mi (0%)     5h18m
  kube-system                 etcd-minikube                                100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         5h18m
  kube-system                 kube-apiserver-minikube                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         5h18m
  kube-system                 kube-controller-manager-minikube             200m (5%)     0 (0%)      0 (0%)           0 (0%)         5h18m
  kube-system                 kube-proxy-4bmgw                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h18m
  kube-system                 kube-scheduler-minikube                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         5h18m
  kube-system                 metrics-server-84c5f94fbc-xrv6m              100m (2%)     0 (0%)      200Mi (0%)       0 (0%)         5h18m
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h18m
  kubernetes-dashboard        dashboard-metrics-scraper-c5db448b4-9x5c7    0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m
  kubernetes-dashboard        kubernetes-dashboard-695b96c756-dgv9h        0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                2950m (73%)  2 (50%)
  memory             500Mi (1%)   400Mi (1%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-1Gi      0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
Events:              <none>


==> dmesg <==
[  +0.000007]  oom_kill_process.cold+0xb/0x10
[  +0.000007]  out_of_memory+0x106/0x2e0
[  +0.000010]  mem_cgroup_out_of_memory+0x13f/0x160
[  +0.000010]  try_charge_memcg+0x687/0x740
[  +0.000008]  charge_memcg+0x45/0xb0
[  +0.000007]  __mem_cgroup_charge+0x2d/0x90
[  +0.000007]  do_anonymous_page+0x114/0x3c0
[  +0.000007]  ? xmit_one.constprop.0+0x99/0x160
[  +0.000007]  handle_pte_fault+0x20a/0x240
[  +0.000008]  __handle_mm_fault+0x405/0x6f0
[  +0.000011]  handle_mm_fault+0xd8/0x2c0
[  +0.000007]  do_user_addr_fault+0x1c9/0x670
[  +0.000009]  exc_page_fault+0x77/0x170
[  +0.000009]  asm_exc_page_fault+0x27/0x30
[  +0.000005] RIP: 0010:copy_user_enhanced_fast_string+0xe/0x40
[  +0.000010] Code: 89 d1 c1 e9 03 83 e2 07 f3 48 a5 89 d1 f3 a4 31 c0 0f 01 ca c3 cc cc cc cc 0f 1f 00 0f 01 cb 83 fa 40 0f 82 70 ff ff ff 89 d1 <f3> a4 31 c0 0f 01 ca c3 cc cc cc cc 66 0f 1f 44 00 00 89 d1 83 f8
[  +0.000004] RSP: 0018:ffffb9418c6c7a20 EFLAGS: 00050206
[  +0.000006] RAX: 000000c001a04000 RBX: 0000000000000099 RCX: 0000000000000099
[  +0.000004] RDX: 0000000000000099 RSI: ffff9956f0778132 RDI: 000000c001a04000
[  +0.000003] RBP: ffffb9418c6c7ac0 R08: 0000000000000099 R09: ffffffff9c6d4f60
[  +0.000004] R10: 0000000000000000 R11: 0000000000000099 R12: ffffb9418c6c7dc0
[  +0.000003] R13: 0000000000000099 R14: 0000000000000000 R15: 0000000000000000
[  +0.000005]  ? receiver_wake_function+0x30/0x30
[  +0.000012]  ? _copy_to_iter+0xd7/0x710
[  +0.000010]  ? __check_object_size.part.0+0x4a/0x150
[  +0.000008]  simple_copy_to_iter+0x39/0x50
[  +0.000008]  __skb_datagram_iter+0x19b/0x2f0
[  +0.000008]  ? receiver_wake_function+0x30/0x30
[  +0.000009]  skb_copy_datagram_iter+0x38/0x80
[  +0.000008]  tcp_recvmsg_locked+0x2a7/0x9e0
[  +0.000011]  tcp_recvmsg+0x79/0x1c0
[  +0.000007]  ? __cond_resched+0x1a/0x50
[  +0.000006]  ? aa_sk_perm+0x43/0x1c0
[  +0.000006]  inet6_recvmsg+0x5c/0x120
[  +0.000008]  ? security_socket_recvmsg+0x3a/0x60
[  +0.000006]  sock_recvmsg+0x58/0x80
[  +0.000007]  sock_read_iter+0x8f/0xf0
[  +0.000008]  new_sync_read+0x17e/0x190
[  +0.000008]  vfs_read+0x103/0x1a0
[  +0.000005]  ksys_read+0xb5/0xf0
[  +0.000005]  ? do_syscall_64+0x69/0xc0
[  +0.000007]  __x64_sys_read+0x19/0x20
[  +0.000005]  do_syscall_64+0x59/0xc0
[  +0.000005]  ? syscall_exit_to_user_mode+0x27/0x50
[  +0.000008]  ? do_syscall_64+0x69/0xc0
[  +0.000005]  ? do_syscall_64+0x69/0xc0
[  +0.000005]  ? exit_to_user_mode_prepare+0x96/0xb0
[  +0.000007]  ? syscall_exit_to_user_mode+0x27/0x50
[  +0.000007]  ? do_syscall_64+0x69/0xc0
[  +0.000006]  entry_SYSCALL_64_after_hwframe+0x61/0xcb
[  +0.000008] RIP: 0033:0x403eae
[  +0.000006] Code: 48 83 ec 38 e8 13 00 00 00 48 83 c4 38 5d c3 cc cc cc cc cc cc cc cc cc cc cc cc cc 49 89 f2 48 89 fa 48 89 ce 48 89 df 0f 05 <48> 3d 01 f0 ff ff 76 15 48 f7 d8 48 89 c1 48 c7 c0 ff ff ff ff 48
[  +0.000004] RSP: 002b:000000c0018a9458 EFLAGS: 00000206 ORIG_RAX: 0000000000000000
[  +0.000006] RAX: ffffffffffffffda RBX: 000000000000063d RCX: 0000000000403eae
[  +0.000004] RDX: 0000000000001000 RSI: 000000c001a04000 RDI: 000000000000063d
[  +0.000004] RBP: 000000c0018a9498 R08: 0000000000000000 R09: 0000000000000000
[  +0.000003] R10: 0000000000000000 R11: 0000000000000206 R12: 000000c0001b5e70
[  +0.000003] R13: 0000000000000001 R14: 000000c00190b180 R15: 0000000000000003
[  +0.000008]  </TASK>
[  +0.000120] Memory cgroup out of memory: Killed process 113507 (app) total-vm:1233364kB, anon-rss:26300kB, file-rss:6164kB, shmem-rss:0kB, UID:0 pgtables:144kB oom_score_adj:-997


==> etcd [fdb36f89f678] <==
{"level":"info","ts":"2025-01-04T13:40:43.549466Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3654728800,"revision":11683,"compact-revision":11442}
{"level":"info","ts":"2025-01-04T13:45:43.554757Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11927}
{"level":"info","ts":"2025-01-04T13:45:43.621678Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":11927,"took":"66.665213ms","hash":1784460040,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":999424,"current-db-size-in-use":"999 kB"}
{"level":"info","ts":"2025-01-04T13:45:43.621771Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1784460040,"revision":11927,"compact-revision":11683}
{"level":"info","ts":"2025-01-04T13:50:43.575254Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12172}
{"level":"info","ts":"2025-01-04T13:50:43.597979Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":12172,"took":"22.279551ms","hash":4023336181,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1110016,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2025-01-04T13:50:43.598020Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4023336181,"revision":12172,"compact-revision":11927}
{"level":"info","ts":"2025-01-04T13:55:43.596421Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12422}
{"level":"info","ts":"2025-01-04T13:55:43.608870Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":12422,"took":"12.029872ms","hash":2838896220,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1196032,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-01-04T13:55:43.608954Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2838896220,"revision":12422,"compact-revision":12172}
{"level":"info","ts":"2025-01-04T14:00:43.616215Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12662}
{"level":"info","ts":"2025-01-04T14:00:43.625077Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":12662,"took":"8.401817ms","hash":2627962547,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":987136,"current-db-size-in-use":"987 kB"}
{"level":"info","ts":"2025-01-04T14:00:43.625162Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2627962547,"revision":12662,"compact-revision":12422}
{"level":"info","ts":"2025-01-04T14:05:43.640260Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12900}
{"level":"info","ts":"2025-01-04T14:05:43.649774Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":12900,"took":"8.854252ms","hash":4253303096,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1011712,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2025-01-04T14:05:43.649859Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4253303096,"revision":12900,"compact-revision":12662}
{"level":"info","ts":"2025-01-04T14:09:16.868681Z","caller":"traceutil/trace.go:171","msg":"trace[115839864] linearizableReadLoop","detail":"{readStateIndex:16576; appliedIndex:16575; }","duration":"121.621836ms","start":"2025-01-04T14:09:16.747049Z","end":"2025-01-04T14:09:16.868671Z","steps":["trace[115839864] 'read index received'  (duration: 121.565607ms)","trace[115839864] 'applied index is now lower than readState.Index'  (duration: 55.807¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-01-04T14:09:16.868740Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"121.680659ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/default/testapp\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-01-04T14:09:16.868782Z","caller":"traceutil/trace.go:171","msg":"trace[1719170706] range","detail":"{range_begin:/registry/deployments/default/testapp; range_end:; response_count:0; response_revision:13337; }","duration":"121.727225ms","start":"2025-01-04T14:09:16.747046Z","end":"2025-01-04T14:09:16.868773Z","steps":["trace[1719170706] 'agreement among raft nodes before linearized reading'  (duration: 121.66515ms)"],"step_count":1}
{"level":"info","ts":"2025-01-04T14:10:43.668021Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13155}
{"level":"info","ts":"2025-01-04T14:10:43.680108Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":13155,"took":"11.586742ms","hash":2826774537,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1040384,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2025-01-04T14:10:43.680201Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2826774537,"revision":13155,"compact-revision":12900}
{"level":"info","ts":"2025-01-04T14:15:43.690597Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13407}
{"level":"info","ts":"2025-01-04T14:15:43.703011Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":13407,"took":"11.776027ms","hash":1582084365,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1040384,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2025-01-04T14:15:43.703092Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1582084365,"revision":13407,"compact-revision":13155}
{"level":"info","ts":"2025-01-04T14:16:27.430925Z","caller":"traceutil/trace.go:171","msg":"trace[387027607] transaction","detail":"{read_only:false; response_revision:13689; number_of_response:1; }","duration":"385.768452ms","start":"2025-01-04T14:16:27.045142Z","end":"2025-01-04T14:16:27.430910Z","steps":["trace[387027607] 'process raft request'  (duration: 385.650091ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-04T14:16:27.431548Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-01-04T14:16:27.045131Z","time spent":"386.07453ms","remote":"127.0.0.1:39040","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:13686 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-01-04T14:20:43.708417Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13651}
{"level":"info","ts":"2025-01-04T14:20:43.718926Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":13651,"took":"9.865955ms","hash":1945770355,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1044480,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2025-01-04T14:20:43.719001Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1945770355,"revision":13651,"compact-revision":13407}
{"level":"info","ts":"2025-01-04T14:25:43.728515Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13895}
{"level":"info","ts":"2025-01-04T14:25:43.738544Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":13895,"took":"9.577984ms","hash":3542433225,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1523712,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-01-04T14:25:43.738631Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3542433225,"revision":13895,"compact-revision":13651}
{"level":"info","ts":"2025-01-04T14:30:43.750886Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14217}
{"level":"info","ts":"2025-01-04T14:30:43.765607Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":14217,"took":"13.892822ms","hash":3278259597,"current-db-size-bytes":2080768,"current-db-size":"2.1 MB","current-db-size-in-use-bytes":1720320,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-01-04T14:30:43.765692Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3278259597,"revision":14217,"compact-revision":13895}
{"level":"info","ts":"2025-01-04T14:33:50.357063Z","caller":"traceutil/trace.go:171","msg":"trace[715117791] transaction","detail":"{read_only:false; response_revision:14611; number_of_response:1; }","duration":"302.903601ms","start":"2025-01-04T14:33:50.054149Z","end":"2025-01-04T14:33:50.357052Z","steps":["trace[715117791] 'process raft request'  (duration: 302.82871ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-04T14:33:50.357295Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-01-04T14:33:50.054138Z","time spent":"303.104724ms","remote":"127.0.0.1:39040","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:14609 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-01-04T14:35:43.766883Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14460}
{"level":"info","ts":"2025-01-04T14:35:43.772359Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":14460,"took":"5.286255ms","hash":3585973987,"current-db-size-bytes":2080768,"current-db-size":"2.1 MB","current-db-size-in-use-bytes":1150976,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-01-04T14:35:43.772532Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3585973987,"revision":14460,"compact-revision":14217}
{"level":"info","ts":"2025-01-04T14:40:43.789151Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14703}
{"level":"info","ts":"2025-01-04T14:40:43.797283Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":14703,"took":"7.271703ms","hash":3908816497,"current-db-size-bytes":2080768,"current-db-size":"2.1 MB","current-db-size-in-use-bytes":1175552,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-01-04T14:40:43.797359Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3908816497,"revision":14703,"compact-revision":14460}
{"level":"info","ts":"2025-01-04T14:45:43.809525Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14948}
{"level":"info","ts":"2025-01-04T14:45:43.820236Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":14948,"took":"10.225824ms","hash":3176653010,"current-db-size-bytes":2199552,"current-db-size":"2.2 MB","current-db-size-in-use-bytes":1286144,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2025-01-04T14:45:43.820312Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3176653010,"revision":14948,"compact-revision":14703}
{"level":"info","ts":"2025-01-04T14:50:43.838610Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15193}
{"level":"info","ts":"2025-01-04T14:50:43.850571Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":15193,"took":"11.114026ms","hash":464299225,"current-db-size-bytes":2322432,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1536000,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-01-04T14:50:43.850656Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":464299225,"revision":15193,"compact-revision":14948}
{"level":"info","ts":"2025-01-04T14:52:45.468915Z","caller":"traceutil/trace.go:171","msg":"trace[907329220] transaction","detail":"{read_only:false; response_revision:15541; number_of_response:1; }","duration":"120.111397ms","start":"2025-01-04T14:52:45.348790Z","end":"2025-01-04T14:52:45.468902Z","steps":["trace[907329220] 'process raft request'  (duration: 120.051461ms)"],"step_count":1}
{"level":"info","ts":"2025-01-04T14:55:43.862978Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15442}
{"level":"info","ts":"2025-01-04T14:55:43.873142Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":15442,"took":"9.706978ms","hash":2197902158,"current-db-size-bytes":2322432,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1462272,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-01-04T14:55:43.873225Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2197902158,"revision":15442,"compact-revision":15193}
{"level":"info","ts":"2025-01-04T15:00:43.884201Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15706}
{"level":"info","ts":"2025-01-04T15:00:43.895362Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":15706,"took":"10.595014ms","hash":653406943,"current-db-size-bytes":2322432,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1564672,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-01-04T15:00:43.895450Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":653406943,"revision":15706,"compact-revision":15442}
{"level":"info","ts":"2025-01-04T15:02:18.885755Z","caller":"etcdserver/server.go:1451","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000}
{"level":"info","ts":"2025-01-04T15:02:18.898525Z","caller":"etcdserver/server.go:2471","msg":"saved snapshot","snapshot-index":20002}
{"level":"info","ts":"2025-01-04T15:02:18.898748Z","caller":"etcdserver/server.go:2501","msg":"compacted Raft logs","compact-index":15002}


==> kernel <==
 15:04:01 up  6:55,  0 users,  load average: 1.48, 1.88, 2.35
Linux minikube 5.15.0-53-generic #59-Ubuntu SMP Mon Oct 17 18:53:30 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [ce20a7eabbeb] <==
E0104 09:45:49.164763       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0104 09:45:49.165940       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0104 09:45:49.166043       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0104 09:45:51.590607       1 handler_proxy.go:99] no RequestInfo found in the context
E0104 09:45:51.590782       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0104 09:45:51.812514       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0104 09:45:51.962296       1 controller.go:615] quota admission added evaluator for: replicasets.apps
W0104 09:45:52.593197       1 handler_proxy.go:99] no RequestInfo found in the context
W0104 09:45:52.593203       1 handler_proxy.go:99] no RequestInfo found in the context
E0104 09:45:52.593291       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E0104 09:45:52.593318       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0104 09:45:52.594421       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0104 09:45:52.594443       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0104 09:46:01.405812       1 handler_proxy.go:99] no RequestInfo found in the context
E0104 09:46:01.405833       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.99.160.39:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.99.160.39:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.99.160.39:443: connect: connection refused" logger="UnhandledError"
E0104 09:46:01.405915       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E0104 09:46:01.407901       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.99.160.39:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.99.160.39:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.99.160.39:443: connect: connection refused" logger="UnhandledError"
E0104 09:46:01.413597       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.99.160.39:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.99.160.39:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.99.160.39:443: connect: connection refused" logger="UnhandledError"
W0104 09:46:02.408814       1 handler_proxy.go:99] no RequestInfo found in the context
W0104 09:46:02.408882       1 handler_proxy.go:99] no RequestInfo found in the context
E0104 09:46:02.408898       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
E0104 09:46:02.409050       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0104 09:46:02.410126       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0104 09:46:02.410249       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0104 09:46:06.449501       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.99.160.39:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.99.160.39:443/apis/metrics.k8s.io/v1beta1\": context deadline exceeded" logger="UnhandledError"
W0104 09:46:06.449922       1 handler_proxy.go:99] no RequestInfo found in the context
E0104 09:46:06.450035       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0104 09:46:06.482481       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0104 11:46:03.012773       1 alloc.go:330] "allocated clusterIPs" service="default/testapp" clusterIPs={"IPv4":"10.96.97.91"}
E0104 11:52:22.169254       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:33834: use of closed network connection
E0104 11:52:46.921324       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:40128: use of closed network connection
I0104 12:08:30.621315       1 alloc.go:330] "allocated clusterIPs" service="default/testapp" clusterIPs={"IPv4":"10.102.121.0"}
I0104 12:13:29.559906       1 alloc.go:330] "allocated clusterIPs" service="default/testapp-svc" clusterIPs={"IPv4":"10.106.219.213"}
I0104 13:15:36.087602       1 alloc.go:330] "allocated clusterIPs" service="default/testapp" clusterIPs={"IPv4":"10.109.188.194"}
I0104 13:26:35.963880       1 alloc.go:330] "allocated clusterIPs" service="default/testapp" clusterIPs={"IPv4":"10.110.125.95"}
I0104 13:30:41.313252       1 alloc.go:330] "allocated clusterIPs" service="default/testapp" clusterIPs={"IPv4":"10.99.44.96"}
I0104 13:42:57.236127       1 alloc.go:330] "allocated clusterIPs" service="default/testapp" clusterIPs={"IPv4":"10.99.96.158"}
I0104 13:46:40.375398       1 alloc.go:330] "allocated clusterIPs" service="default/testapp" clusterIPs={"IPv4":"10.108.238.93"}
I0104 14:02:16.404243       1 controller.go:615] quota admission added evaluator for: horizontalpodautoscalers.autoscaling
I0104 14:24:31.485550       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/kubernetes-dashboard" clusterIPs={"IPv4":"10.102.228.213"}
I0104 14:24:31.527395       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/dashboard-metrics-scraper" clusterIPs={"IPv4":"10.110.62.223"}
I0104 14:46:45.643170       1 alloc.go:330] "allocated clusterIPs" service="default/testapp" clusterIPs={"IPv4":"10.97.176.21"}


==> kube-controller-manager [eb8e3bbc0c41] <==
I0104 14:24:31.301594       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="36.232191ms"
E0104 14:24:31.302151       1 replica_set.go:560] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-695b96c756\" failed with pods \"kubernetes-dashboard-695b96c756-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
I0104 14:24:31.307343       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="4.280588ms"
E0104 14:24:31.307363       1 replica_set.go:560] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4\" failed with pods \"dashboard-metrics-scraper-c5db448b4-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
I0104 14:24:31.310130       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="6.90917ms"
E0104 14:24:31.310208       1 replica_set.go:560] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-695b96c756\" failed with pods \"kubernetes-dashboard-695b96c756-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
I0104 14:24:31.314524       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="5.321499ms"
E0104 14:24:31.314545       1 replica_set.go:560] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4\" failed with pods \"dashboard-metrics-scraper-c5db448b4-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
I0104 14:24:31.336400       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="24.689253ms"
I0104 14:24:31.360267       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="22.637177ms"
I0104 14:24:31.409688       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="73.241065ms"
I0104 14:24:31.410348       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="112.327¬µs"
I0104 14:24:31.437803       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="77.016744ms"
I0104 14:24:31.438012       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="99.221¬µs"
I0104 14:24:38.849599       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="11.688415ms"
I0104 14:24:38.850974       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="85.519¬µs"
I0104 14:24:51.031445       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="15.413413ms"
I0104 14:24:51.031624       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="94.61¬µs"
I0104 14:24:59.384569       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0104 14:30:05.898318       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0104 14:35:11.571654       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0104 14:40:18.038427       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0104 14:45:23.328065       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0104 14:50:28.967962       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0104 14:55:18.582136       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="41.419994ms"
I0104 14:55:18.582472       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="42.619¬µs"
I0104 14:55:19.588875       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="9.948336ms"
I0104 14:55:19.589387       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="50.964¬µs"
I0104 14:55:33.715459       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="21.447762ms"
I0104 14:55:33.715498       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="19.428¬µs"
I0104 14:55:36.603666       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0104 14:55:46.943286       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="43.282¬µs"
I0104 14:55:47.932765       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="19.599777ms"
I0104 14:55:47.934561       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="1.723944ms"
I0104 14:55:57.016544       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="9.713756ms"
I0104 14:55:57.016831       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="29.382¬µs"
E0104 14:56:03.234095       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/testapp-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get memory resource metric value: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API" logger="UnhandledError"
I0104 14:56:07.925236       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="80.38¬µs"
E0104 14:56:18.261098       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/testapp-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get memory resource metric value: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API" logger="UnhandledError"
I0104 14:56:23.479760       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="19.554483ms"
I0104 14:56:23.479901       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="76.174¬µs"
I0104 14:56:32.648241       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="8.412249ms"
I0104 14:56:32.648299       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="23.059¬µs"
E0104 14:56:33.268994       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/testapp-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get memory resource metric value: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API" logger="UnhandledError"
I0104 14:56:47.928279       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="30.578¬µs"
E0104 14:56:48.297660       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/testapp-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get memory resource metric value: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API" logger="UnhandledError"
E0104 14:57:03.313531       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/testapp-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get memory resource metric value: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API" logger="UnhandledError"
I0104 14:57:13.356100       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="13.819779ms"
I0104 14:57:13.356178       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="41.189¬µs"
E0104 14:57:18.341079       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/testapp-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get memory resource metric value: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API" logger="UnhandledError"
E0104 14:57:33.358806       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/testapp-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get memory resource metric value: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API" logger="UnhandledError"
E0104 14:57:48.384675       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/testapp-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get memory resource metric value: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API" logger="UnhandledError"
I0104 15:00:03.581816       1 horizontal.go:886] "Successfully rescaled" logger="horizontal-pod-autoscaler-controller" HPA="default/testapp-hpa" currentReplicas=1 desiredReplicas=2 reason="memory resource utilization (percentage of request) above target"
I0104 15:00:03.663285       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="65.587071ms"
I0104 15:00:03.707058       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="42.6071ms"
I0104 15:00:03.742587       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="35.419978ms"
I0104 15:00:03.743410       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="750.589¬µs"
I0104 15:00:04.859444       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="21.433729ms"
I0104 15:00:04.861567       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/testapp-deployment-8585d4875f" duration="88.935¬µs"
I0104 15:00:42.809256       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [3180d4c0f670] <==
I0104 09:45:53.072029       1 server_linux.go:66] "Using iptables proxy"
I0104 09:45:53.184556       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0104 09:45:53.184600       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0104 09:45:53.200307       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0104 09:45:53.200348       1 server_linux.go:169] "Using iptables Proxier"
I0104 09:45:53.202504       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0104 09:45:53.202744       1 server.go:483] "Version info" version="v1.31.0"
I0104 09:45:53.202757       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0104 09:45:53.203827       1 config.go:197] "Starting service config controller"
I0104 09:45:53.203855       1 shared_informer.go:313] Waiting for caches to sync for service config
I0104 09:45:53.203895       1 config.go:104] "Starting endpoint slice config controller"
I0104 09:45:53.203900       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0104 09:45:53.204263       1 config.go:326] "Starting node config controller"
I0104 09:45:53.204271       1 shared_informer.go:313] Waiting for caches to sync for node config
I0104 09:45:53.304724       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0104 09:45:53.304756       1 shared_informer.go:320] Caches are synced for service config
I0104 09:45:53.305127       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [b3cb1c12d665] <==
E0104 09:45:43.273666       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0104 09:45:43.273677       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0104 09:45:43.273692       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0104 09:45:43.273745       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0104 09:45:43.273758       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0104 09:45:43.273810       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0104 09:45:43.273821       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0104 09:45:43.273869       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0104 09:45:43.273877       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0104 09:45:43.273886       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0104 09:45:43.273901       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0104 09:45:43.273927       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0104 09:45:43.273936       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0104 09:45:43.273978       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0104 09:45:43.273986       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0104 09:45:43.274029       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0104 09:45:43.274037       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0104 09:45:43.274107       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0104 09:45:43.274118       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0104 09:45:43.274155       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0104 09:45:43.274166       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0104 09:45:43.274194       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0104 09:45:43.274203       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0104 09:45:43.274281       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0104 09:45:43.274296       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0104 09:45:43.274325       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0104 09:45:43.274406       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0104 09:45:43.274344       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0104 09:45:43.274603       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0104 09:45:44.140450       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0104 09:45:44.140527       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0104 09:45:44.223106       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0104 09:45:44.223184       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0104 09:45:44.257076       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0104 09:45:44.257555       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0104 09:45:44.260611       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0104 09:45:44.260687       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0104 09:45:44.406482       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0104 09:45:44.406571       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0104 09:45:44.440621       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0104 09:45:44.440697       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0104 09:45:44.504122       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0104 09:45:44.504203       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0104 09:45:44.531271       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0104 09:45:44.531359       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0104 09:45:44.571369       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0104 09:45:44.571450       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0104 09:45:44.643524       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0104 09:45:44.643946       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0104 09:45:44.671724       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0104 09:45:44.671947       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0104 09:45:44.683099       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0104 09:45:44.683184       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0104 09:45:44.781060       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0104 09:45:44.781893       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0104 09:45:44.781776       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0104 09:45:44.782038       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0104 09:45:44.834759       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0104 09:45:44.834833       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
I0104 09:45:47.172309       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Jan 04 10:13:13 minikube kubelet[2859]: I0104 10:13:13.475308    2859 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/75e0caaf-c57b-4129-bcef-8c6ba1d73b1b-kube-api-access-75vj5" (OuterVolumeSpecName: "kube-api-access-75vj5") pod "75e0caaf-c57b-4129-bcef-8c6ba1d73b1b" (UID: "75e0caaf-c57b-4129-bcef-8c6ba1d73b1b"). InnerVolumeSpecName "kube-api-access-75vj5". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jan 04 10:13:13 minikube kubelet[2859]: I0104 10:13:13.475427    2859 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/babfda1e-c0a7-4515-b9cc-635697d495fe-kube-api-access-926hf" (OuterVolumeSpecName: "kube-api-access-926hf") pod "babfda1e-c0a7-4515-b9cc-635697d495fe" (UID: "babfda1e-c0a7-4515-b9cc-635697d495fe"). InnerVolumeSpecName "kube-api-access-926hf". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jan 04 10:13:13 minikube kubelet[2859]: I0104 10:13:13.476188    2859 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/f52600c2-74bd-4867-8e49-24a7b16797f2-kube-api-access-crd52" (OuterVolumeSpecName: "kube-api-access-crd52") pod "f52600c2-74bd-4867-8e49-24a7b16797f2" (UID: "f52600c2-74bd-4867-8e49-24a7b16797f2"). InnerVolumeSpecName "kube-api-access-crd52". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jan 04 10:13:13 minikube kubelet[2859]: I0104 10:13:13.571980    2859 reconciler_common.go:288] "Volume detached for volume \"kube-api-access-926hf\" (UniqueName: \"kubernetes.io/projected/babfda1e-c0a7-4515-b9cc-635697d495fe-kube-api-access-926hf\") on node \"minikube\" DevicePath \"\""
Jan 04 10:13:13 minikube kubelet[2859]: I0104 10:13:13.572473    2859 reconciler_common.go:288] "Volume detached for volume \"kube-api-access-crd52\" (UniqueName: \"kubernetes.io/projected/f52600c2-74bd-4867-8e49-24a7b16797f2-kube-api-access-crd52\") on node \"minikube\" DevicePath \"\""
Jan 04 10:13:13 minikube kubelet[2859]: I0104 10:13:13.572523    2859 reconciler_common.go:288] "Volume detached for volume \"kube-api-access-75vj5\" (UniqueName: \"kubernetes.io/projected/75e0caaf-c57b-4129-bcef-8c6ba1d73b1b-kube-api-access-75vj5\") on node \"minikube\" DevicePath \"\""
Jan 04 10:13:14 minikube kubelet[2859]: I0104 10:13:14.441929    2859 scope.go:117] "RemoveContainer" containerID="e842e0c688408bf58eb8b32754b64cedaf941c71d1204657cef7b13e4f806c55"
Jan 04 10:13:14 minikube kubelet[2859]: I0104 10:13:14.924017    2859 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="75e0caaf-c57b-4129-bcef-8c6ba1d73b1b" path="/var/lib/kubelet/pods/75e0caaf-c57b-4129-bcef-8c6ba1d73b1b/volumes"
Jan 04 10:13:14 minikube kubelet[2859]: I0104 10:13:14.926646    2859 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="babfda1e-c0a7-4515-b9cc-635697d495fe" path="/var/lib/kubelet/pods/babfda1e-c0a7-4515-b9cc-635697d495fe/volumes"
Jan 04 10:13:14 minikube kubelet[2859]: I0104 10:13:14.930047    2859 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="f52600c2-74bd-4867-8e49-24a7b16797f2" path="/var/lib/kubelet/pods/f52600c2-74bd-4867-8e49-24a7b16797f2/volumes"
Jan 04 10:13:14 minikube kubelet[2859]: I0104 10:13:14.932802    2859 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="ff164eae-f696-40de-8c46-f582e75dc5b8" path="/var/lib/kubelet/pods/ff164eae-f696-40de-8c46-f582e75dc5b8/volumes"
Jan 04 10:13:26 minikube kubelet[2859]: I0104 10:13:26.167603    2859 reconciler_common.go:159] "operationExecutor.UnmountVolume started for volume \"kube-api-access-c8kf8\" (UniqueName: \"kubernetes.io/projected/8c21caab-fd2d-4dd9-9faa-1f7754dd6aaf-kube-api-access-c8kf8\") pod \"8c21caab-fd2d-4dd9-9faa-1f7754dd6aaf\" (UID: \"8c21caab-fd2d-4dd9-9faa-1f7754dd6aaf\") "
Jan 04 10:13:26 minikube kubelet[2859]: I0104 10:13:26.173499    2859 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/8c21caab-fd2d-4dd9-9faa-1f7754dd6aaf-kube-api-access-c8kf8" (OuterVolumeSpecName: "kube-api-access-c8kf8") pod "8c21caab-fd2d-4dd9-9faa-1f7754dd6aaf" (UID: "8c21caab-fd2d-4dd9-9faa-1f7754dd6aaf"). InnerVolumeSpecName "kube-api-access-c8kf8". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jan 04 10:13:26 minikube kubelet[2859]: I0104 10:13:26.268806    2859 reconciler_common.go:288] "Volume detached for volume \"kube-api-access-c8kf8\" (UniqueName: \"kubernetes.io/projected/8c21caab-fd2d-4dd9-9faa-1f7754dd6aaf-kube-api-access-c8kf8\") on node \"minikube\" DevicePath \"\""
Jan 04 10:13:26 minikube kubelet[2859]: I0104 10:13:26.644230    2859 scope.go:117] "RemoveContainer" containerID="a3dcca151dca4072982363d4a3fa84201b1b58fef271bc67cc5222999964a4ee"
Jan 04 10:13:26 minikube kubelet[2859]: I0104 10:13:26.685677    2859 scope.go:117] "RemoveContainer" containerID="a3dcca151dca4072982363d4a3fa84201b1b58fef271bc67cc5222999964a4ee"
Jan 04 10:13:26 minikube kubelet[2859]: E0104 10:13:26.687234    2859 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: a3dcca151dca4072982363d4a3fa84201b1b58fef271bc67cc5222999964a4ee" containerID="a3dcca151dca4072982363d4a3fa84201b1b58fef271bc67cc5222999964a4ee"
Jan 04 10:13:26 minikube kubelet[2859]: I0104 10:13:26.687290    2859 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"a3dcca151dca4072982363d4a3fa84201b1b58fef271bc67cc5222999964a4ee"} err="failed to get container status \"a3dcca151dca4072982363d4a3fa84201b1b58fef271bc67cc5222999964a4ee\": rpc error: code = Unknown desc = Error response from daemon: No such container: a3dcca151dca4072982363d4a3fa84201b1b58fef271bc67cc5222999964a4ee"
Jan 04 10:13:26 minikube kubelet[2859]: I0104 10:13:26.922025    2859 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="8c21caab-fd2d-4dd9-9faa-1f7754dd6aaf" path="/var/lib/kubelet/pods/8c21caab-fd2d-4dd9-9faa-1f7754dd6aaf/volumes"
Jan 04 10:39:38 minikube kubelet[2859]: E0104 10:39:38.946442    2859 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="8c21caab-fd2d-4dd9-9faa-1f7754dd6aaf" containerName="scaletestapp"
Jan 04 10:39:38 minikube kubelet[2859]: E0104 10:39:38.946464    2859 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="ff164eae-f696-40de-8c46-f582e75dc5b8" containerName="scaletestapp"
Jan 04 10:39:38 minikube kubelet[2859]: E0104 10:39:38.946471    2859 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="75e0caaf-c57b-4129-bcef-8c6ba1d73b1b" containerName="scaletestapp"
Jan 04 10:39:38 minikube kubelet[2859]: E0104 10:39:38.946476    2859 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="babfda1e-c0a7-4515-b9cc-635697d495fe" containerName="scaletestapp"
Jan 04 10:39:38 minikube kubelet[2859]: E0104 10:39:38.946481    2859 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="f52600c2-74bd-4867-8e49-24a7b16797f2" containerName="scaletestapp"
Jan 04 10:39:38 minikube kubelet[2859]: I0104 10:39:38.946500    2859 memory_manager.go:354] "RemoveStaleState removing state" podUID="8c21caab-fd2d-4dd9-9faa-1f7754dd6aaf" containerName="scaletestapp"
Jan 04 10:39:38 minikube kubelet[2859]: I0104 10:39:38.946506    2859 memory_manager.go:354] "RemoveStaleState removing state" podUID="f52600c2-74bd-4867-8e49-24a7b16797f2" containerName="scaletestapp"
Jan 04 10:39:38 minikube kubelet[2859]: I0104 10:39:38.946511    2859 memory_manager.go:354] "RemoveStaleState removing state" podUID="babfda1e-c0a7-4515-b9cc-635697d495fe" containerName="scaletestapp"
Jan 04 10:39:38 minikube kubelet[2859]: I0104 10:39:38.946518    2859 memory_manager.go:354] "RemoveStaleState removing state" podUID="75e0caaf-c57b-4129-bcef-8c6ba1d73b1b" containerName="scaletestapp"
Jan 04 10:39:38 minikube kubelet[2859]: I0104 10:39:38.946522    2859 memory_manager.go:354] "RemoveStaleState removing state" podUID="ff164eae-f696-40de-8c46-f582e75dc5b8" containerName="scaletestapp"
Jan 04 10:39:38 minikube kubelet[2859]: W0104 10:39:38.947914    2859 reflector.go:561] object-"default"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "default": no relationship found between node 'minikube' and this object
Jan 04 10:39:38 minikube kubelet[2859]: E0104 10:39:38.947945    2859 reflector.go:158] "Unhandled Error" err="object-\"default\"/\"kube-root-ca.crt\": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"kube-root-ca.crt\" is forbidden: User \"system:node:minikube\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"default\": no relationship found between node 'minikube' and this object" logger="UnhandledError"
Jan 04 10:39:39 minikube kubelet[2859]: I0104 10:39:39.011711    2859 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mjlhn\" (UniqueName: \"kubernetes.io/projected/69513d57-a709-4dc1-904f-03b43e0c6104-kube-api-access-mjlhn\") pod \"testapp-deployment-8585d4875f-2mwdd\" (UID: \"69513d57-a709-4dc1-904f-03b43e0c6104\") " pod="default/testapp-deployment-8585d4875f-2mwdd"
Jan 04 14:24:31 minikube kubelet[2859]: I0104 14:24:31.335220    2859 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/testapp-deployment-8585d4875f-2mwdd" podStartSLOduration=13493.335207521 podStartE2EDuration="3h44m53.335207521s" podCreationTimestamp="2025-01-04 10:39:38 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-01-04 10:39:41.671712092 +0000 UTC m=+3235.044679999" watchObservedRunningTime="2025-01-04 14:24:31.335207521 +0000 UTC m=+16724.708175285"
Jan 04 14:24:31 minikube kubelet[2859]: I0104 14:24:31.503753    2859 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-lsqtf\" (UniqueName: \"kubernetes.io/projected/5bd427f0-8bb3-40a5-b751-bbb4e529e8f4-kube-api-access-lsqtf\") pod \"kubernetes-dashboard-695b96c756-dgv9h\" (UID: \"5bd427f0-8bb3-40a5-b751-bbb4e529e8f4\") " pod="kubernetes-dashboard/kubernetes-dashboard-695b96c756-dgv9h"
Jan 04 14:24:31 minikube kubelet[2859]: I0104 14:24:31.504185    2859 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-fbgqf\" (UniqueName: \"kubernetes.io/projected/7b331a19-7138-4b04-8d6f-8462b73fd3bd-kube-api-access-fbgqf\") pod \"dashboard-metrics-scraper-c5db448b4-9x5c7\" (UID: \"7b331a19-7138-4b04-8d6f-8462b73fd3bd\") " pod="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4-9x5c7"
Jan 04 14:24:31 minikube kubelet[2859]: I0104 14:24:31.504551    2859 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/5bd427f0-8bb3-40a5-b751-bbb4e529e8f4-tmp-volume\") pod \"kubernetes-dashboard-695b96c756-dgv9h\" (UID: \"5bd427f0-8bb3-40a5-b751-bbb4e529e8f4\") " pod="kubernetes-dashboard/kubernetes-dashboard-695b96c756-dgv9h"
Jan 04 14:24:31 minikube kubelet[2859]: I0104 14:24:31.505350    2859 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/7b331a19-7138-4b04-8d6f-8462b73fd3bd-tmp-volume\") pod \"dashboard-metrics-scraper-c5db448b4-9x5c7\" (UID: \"7b331a19-7138-4b04-8d6f-8462b73fd3bd\") " pod="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4-9x5c7"
Jan 04 14:24:38 minikube kubelet[2859]: I0104 14:24:38.834723    2859 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4-9x5c7" podStartSLOduration=2.489626553 podStartE2EDuration="7.834674321s" podCreationTimestamp="2025-01-04 14:24:31 +0000 UTC" firstStartedPulling="2025-01-04 14:24:32.44381507 +0000 UTC m=+16725.816782846" lastFinishedPulling="2025-01-04 14:24:37.788862777 +0000 UTC m=+16731.161830614" observedRunningTime="2025-01-04 14:24:38.833771007 +0000 UTC m=+16732.206738845" watchObservedRunningTime="2025-01-04 14:24:38.834674321 +0000 UTC m=+16732.207642137"
Jan 04 14:24:51 minikube kubelet[2859]: I0104 14:24:51.017362    2859 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kubernetes-dashboard/kubernetes-dashboard-695b96c756-dgv9h" podStartSLOduration=2.694186754 podStartE2EDuration="20.017324486s" podCreationTimestamp="2025-01-04 14:24:31 +0000 UTC" firstStartedPulling="2025-01-04 14:24:32.453667684 +0000 UTC m=+16725.826635460" lastFinishedPulling="2025-01-04 14:24:49.77680537 +0000 UTC m=+16743.149773192" observedRunningTime="2025-01-04 14:24:51.015418951 +0000 UTC m=+16744.388386781" watchObservedRunningTime="2025-01-04 14:24:51.017324486 +0000 UTC m=+16744.390292335"
Jan 04 14:55:18 minikube kubelet[2859]: I0104 14:55:18.525102    2859 scope.go:117] "RemoveContainer" containerID="e28d626b05b85b5305322d030c364e87e47992d349d16f3ae6539516f0ff3d8d"
Jan 04 14:55:33 minikube kubelet[2859]: I0104 14:55:33.670444    2859 scope.go:117] "RemoveContainer" containerID="e28d626b05b85b5305322d030c364e87e47992d349d16f3ae6539516f0ff3d8d"
Jan 04 14:55:33 minikube kubelet[2859]: I0104 14:55:33.670915    2859 scope.go:117] "RemoveContainer" containerID="753c6b85ab321c8b2a2f3a90597b088d7d9e2a9c7f521905ba7c4c6fdce2bf42"
Jan 04 14:55:33 minikube kubelet[2859]: E0104 14:55:33.671044    2859 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"testapp\" with CrashLoopBackOff: \"back-off 10s restarting failed container=testapp pod=testapp-deployment-8585d4875f-2mwdd_default(69513d57-a709-4dc1-904f-03b43e0c6104)\"" pod="default/testapp-deployment-8585d4875f-2mwdd" podUID="69513d57-a709-4dc1-904f-03b43e0c6104"
Jan 04 14:55:46 minikube kubelet[2859]: I0104 14:55:46.906773    2859 scope.go:117] "RemoveContainer" containerID="753c6b85ab321c8b2a2f3a90597b088d7d9e2a9c7f521905ba7c4c6fdce2bf42"
Jan 04 14:55:56 minikube kubelet[2859]: I0104 14:55:56.991549    2859 scope.go:117] "RemoveContainer" containerID="753c6b85ab321c8b2a2f3a90597b088d7d9e2a9c7f521905ba7c4c6fdce2bf42"
Jan 04 14:55:56 minikube kubelet[2859]: I0104 14:55:56.991744    2859 scope.go:117] "RemoveContainer" containerID="46ae829ba739571c5f913a1867ca1ef3dd45490055b139c75251bfb791f3b23f"
Jan 04 14:55:56 minikube kubelet[2859]: E0104 14:55:56.991854    2859 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"testapp\" with CrashLoopBackOff: \"back-off 20s restarting failed container=testapp pod=testapp-deployment-8585d4875f-2mwdd_default(69513d57-a709-4dc1-904f-03b43e0c6104)\"" pod="default/testapp-deployment-8585d4875f-2mwdd" podUID="69513d57-a709-4dc1-904f-03b43e0c6104"
Jan 04 14:56:07 minikube kubelet[2859]: I0104 14:56:07.905835    2859 scope.go:117] "RemoveContainer" containerID="46ae829ba739571c5f913a1867ca1ef3dd45490055b139c75251bfb791f3b23f"
Jan 04 14:56:07 minikube kubelet[2859]: E0104 14:56:07.906296    2859 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"testapp\" with CrashLoopBackOff: \"back-off 20s restarting failed container=testapp pod=testapp-deployment-8585d4875f-2mwdd_default(69513d57-a709-4dc1-904f-03b43e0c6104)\"" pod="default/testapp-deployment-8585d4875f-2mwdd" podUID="69513d57-a709-4dc1-904f-03b43e0c6104"
Jan 04 14:56:22 minikube kubelet[2859]: I0104 14:56:22.905609    2859 scope.go:117] "RemoveContainer" containerID="46ae829ba739571c5f913a1867ca1ef3dd45490055b139c75251bfb791f3b23f"
Jan 04 14:56:32 minikube kubelet[2859]: I0104 14:56:32.623766    2859 scope.go:117] "RemoveContainer" containerID="46ae829ba739571c5f913a1867ca1ef3dd45490055b139c75251bfb791f3b23f"
Jan 04 14:56:32 minikube kubelet[2859]: I0104 14:56:32.624066    2859 scope.go:117] "RemoveContainer" containerID="f7ec1d9dd88e7515eb90d56a2381f7d3e3bbb4de0d36976cdc846ca334cabeca"
Jan 04 14:56:32 minikube kubelet[2859]: E0104 14:56:32.625117    2859 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"testapp\" with CrashLoopBackOff: \"back-off 40s restarting failed container=testapp pod=testapp-deployment-8585d4875f-2mwdd_default(69513d57-a709-4dc1-904f-03b43e0c6104)\"" pod="default/testapp-deployment-8585d4875f-2mwdd" podUID="69513d57-a709-4dc1-904f-03b43e0c6104"
Jan 04 14:56:47 minikube kubelet[2859]: I0104 14:56:47.905212    2859 scope.go:117] "RemoveContainer" containerID="f7ec1d9dd88e7515eb90d56a2381f7d3e3bbb4de0d36976cdc846ca334cabeca"
Jan 04 14:56:47 minikube kubelet[2859]: E0104 14:56:47.905332    2859 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"testapp\" with CrashLoopBackOff: \"back-off 40s restarting failed container=testapp pod=testapp-deployment-8585d4875f-2mwdd_default(69513d57-a709-4dc1-904f-03b43e0c6104)\"" pod="default/testapp-deployment-8585d4875f-2mwdd" podUID="69513d57-a709-4dc1-904f-03b43e0c6104"
Jan 04 14:56:59 minikube kubelet[2859]: I0104 14:56:59.905486    2859 scope.go:117] "RemoveContainer" containerID="f7ec1d9dd88e7515eb90d56a2381f7d3e3bbb4de0d36976cdc846ca334cabeca"
Jan 04 14:56:59 minikube kubelet[2859]: E0104 14:56:59.905832    2859 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"testapp\" with CrashLoopBackOff: \"back-off 40s restarting failed container=testapp pod=testapp-deployment-8585d4875f-2mwdd_default(69513d57-a709-4dc1-904f-03b43e0c6104)\"" pod="default/testapp-deployment-8585d4875f-2mwdd" podUID="69513d57-a709-4dc1-904f-03b43e0c6104"
Jan 04 14:57:11 minikube kubelet[2859]: I0104 14:57:11.905795    2859 scope.go:117] "RemoveContainer" containerID="f7ec1d9dd88e7515eb90d56a2381f7d3e3bbb4de0d36976cdc846ca334cabeca"
Jan 04 15:00:03 minikube kubelet[2859]: I0104 15:00:03.738706    2859 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mktrn\" (UniqueName: \"kubernetes.io/projected/ff2244fe-31b0-4d0f-a6df-a4c8515b7d73-kube-api-access-mktrn\") pod \"testapp-deployment-8585d4875f-wtt96\" (UID: \"ff2244fe-31b0-4d0f-a6df-a4c8515b7d73\") " pod="default/testapp-deployment-8585d4875f-wtt96"
Jan 04 15:00:04 minikube kubelet[2859]: I0104 15:00:04.848270    2859 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/testapp-deployment-8585d4875f-wtt96" podStartSLOduration=1.848232697 podStartE2EDuration="1.848232697s" podCreationTimestamp="2025-01-04 15:00:03 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-01-04 15:00:04.846603021 +0000 UTC m=+18858.219570848" watchObservedRunningTime="2025-01-04 15:00:04.848232697 +0000 UTC m=+18858.221200503"


==> kubernetes-dashboard [f84386dafd39] <==
2025/01/04 15:01:51 [2025-01-04T15:01:51Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:51 [2025-01-04T15:01:51Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:51 [2025-01-04T15:01:51Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:51 Getting list of all pet sets in the cluster
2025/01/04 15:01:51 [2025-01-04T15:01:51Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:51 [2025-01-04T15:01:51Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:54 [2025-01-04T15:01:54Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/01/04 15:01:54 Getting list of namespaces
2025/01/04 15:01:54 [2025-01-04T15:01:54Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:56 [2025-01-04T15:01:56Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:56 Getting list of all cron jobs in the cluster
2025/01/04 15:01:56 [2025-01-04T15:01:56Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:56 [2025-01-04T15:01:56Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:56 Getting list of all deployments in the cluster
2025/01/04 15:01:56 [2025-01-04T15:01:56Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:56 [2025-01-04T15:01:56Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:56 [2025-01-04T15:01:56Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:56 Getting list of all jobs in the cluster
2025/01/04 15:01:56 [2025-01-04T15:01:56Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:56 Getting list of all pods in the cluster
2025/01/04 15:01:56 [2025-01-04T15:01:56Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:56 Getting list of all replica sets in the cluster
2025/01/04 15:01:56 [2025-01-04T15:01:56Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:56 [2025-01-04T15:01:56Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:56 Getting list of all replication controllers in the cluster
2025/01/04 15:01:56 [2025-01-04T15:01:56Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:56 Getting list of all pet sets in the cluster
2025/01/04 15:01:56 [2025-01-04T15:01:56Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:56 [2025-01-04T15:01:56Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:56 Getting pod metrics
2025/01/04 15:01:56 [2025-01-04T15:01:56Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:56 [2025-01-04T15:01:56Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:56 [2025-01-04T15:01:56Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:58 Getting list of all cron jobs in the cluster
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:58 Getting list of all deployments in the cluster
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/01/04 15:01:58 Getting list of namespaces
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:58 Getting list of all jobs in the cluster
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:58 Getting list of all pods in the cluster
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:58 Getting list of all pet sets in the cluster
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:58 Getting list of all replication controllers in the cluster
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/01/04 15:01:58 Getting list of all replica sets in the cluster
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:58 Getting pod metrics
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Outcoming response to 127.0.0.1 with 200 status code
2025/01/04 15:01:58 [2025-01-04T15:01:58Z] Outcoming response to 127.0.0.1 with 200 status code


==> storage-provisioner [47f532f1abd4] <==
I0104 09:45:53.608063       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0104 09:45:53.633837       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0104 09:45:53.634086       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0104 09:45:53.652476       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0104 09:45:53.652817       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"aa528388-05fd-4206-a2ca-3d1cfc448e8b", APIVersion:"v1", ResourceVersion:"428", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_edd64972-fbea-4688-864b-71b62653eaa6 became leader
I0104 09:45:53.652883       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_edd64972-fbea-4688-864b-71b62653eaa6!
I0104 09:45:53.755214       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_edd64972-fbea-4688-864b-71b62653eaa6!

